{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "'''PACKAGE IMPORTS'''\n",
    "#For data analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import xarray as xr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime as dt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from math import log10, floor, pow, e, sqrt\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Data Links'''\n",
    "import_path = \"./data/original-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Colors'''\n",
    "pal = sns.color_palette('GnBu', 5)\n",
    "pal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Functions'''\n",
    "def jitter(values,j):\n",
    "    return values + np.random.normal(j,0.1,values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Data'''\n",
    "### Import Snow Data\n",
    "s2data = xr.open_dataset(import_path + '01_cleanedsnowdataS2.nc')\n",
    "s2data_df = s2data.to_dataframe().reset_index(drop = False)\n",
    "s2data_df = s2data_df.replace('NaN', np.nan)\n",
    "s2data_df = s2data_df.replace('nan', np.nan)\n",
    "s2data_df.time = pd.to_datetime(s2data_df.time)\n",
    "s2data_df['watershed'] = 'S2'\n",
    "\n",
    "s6data = xr.open_dataset(import_path + '01_cleanedsnowdataS6.nc')\n",
    "s6data_df = s6data.to_dataframe().reset_index(drop = False)\n",
    "s6data_df = s6data_df.replace('NaN', np.nan)\n",
    "s6data_df = s6data_df.replace('nan', np.nan)\n",
    "s6data_df.time = pd.to_datetime(s6data_df.time)\n",
    "s6data_df['watershed'] = 'S6'\n",
    "\n",
    "allSnow_df = pd.concat([s6data_df, s2data_df]).reset_index()\n",
    "\n",
    "### Import LAI Data\n",
    "s2LAI = pd.read_csv(import_path + \"S2_winterLAI_calibrated.csv\")\n",
    "s6LAI = pd.read_csv(import_path + \"S6_winterLAI_calibrated.csv\")\n",
    "lai_df = pd.concat([s2LAI, s6LAI]).reset_index()\n",
    "\n",
    "### Import forestry Data -- summarized by stake\n",
    "forestInv = pd.read_csv(import_path + 'VegData_LAI_Snow.csv')\n",
    "\n",
    "### Import Weekly Met Data\n",
    "weeklyMet = pd.read_csv(import_path + \"WeeklyATMSummary.csv\")\n",
    "\n",
    "# Import SWE data\n",
    "SWE = pd.read_csv(import_path + \"2023SWE_data.csv\", \n",
    "                parse_dates = ['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add Snow Year'''\n",
    "#Snow year\n",
    "allSnow_df['SYear'] = [2024 if allSnow_df.time[i] > pd.to_datetime('09-01-2023') else 2023 for i in range(0, len(allSnow_df.time))]\n",
    "\n",
    "#Date of snow year\n",
    "allSnow_df['SDOY'] = [(allSnow_df.time[i] - pd.to_datetime('09-01-2023')).days if allSnow_df.SYear[i] == 2024 else (allSnow_df.time[i] - pd.to_datetime('09-01-2022')).days for i in range(0, len(allSnow_df.time))]\n",
    "allSnow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrofit one veg data outlier\n",
    "#Stake S234 -- labelled all bog trees at dominant when they probably should have been codominant\n",
    "forestInv.loc[18,  'Co'] = [20]\n",
    "forestInv.loc[18,  'Dom'] = [0]\n",
    "forestInv[forestInv.Stake_ID == 'S234']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Organize all data into one data frame'''\n",
    "#Remove rows on allSnow containing NaNs (these are grid locations where data was not taken)\n",
    "allSnow_nan = allSnow_df.dropna(how = 'any')\n",
    "\n",
    "#Take peak snow depth per site\n",
    "peakSnow = allSnow_nan.groupby(['stakes', 'northing', 'easting', 'zones', 'aspect', 'slope', 'watershed', 'SYear'])['depths'].max().reset_index()\n",
    "peakSnow = peakSnow.rename(columns = {'depths' : 'maxDepth'})\n",
    "\n",
    "#Take average snow depth per site\n",
    "avgSnow = allSnow_nan.groupby(['stakes', 'northing', 'easting', 'zones', 'aspect', 'slope', 'watershed', 'SYear'])['depths'].mean().reset_index()\n",
    "avgSnow = avgSnow.rename(columns = {'depths' : 'meanDepth'})\n",
    "\n",
    "#Add in other columns aggregated by year, joined on stakes\n",
    "## LAI\n",
    "lai_sub = lai_df[['Stake_ID', 'OLS Prediction Ring 5']]\n",
    "peakData = pd.merge(peakSnow, lai_sub, left_on = 'stakes', right_on = 'Stake_ID')\n",
    "peakData = peakData.drop(columns = 'Stake_ID')\n",
    "peakData = peakData.rename(columns = {'OLS Prediction Ring 5' : 'lai5ring'})\n",
    "\n",
    "avgData = pd.merge(avgSnow, lai_sub, left_on = 'stakes', right_on = 'Stake_ID')\n",
    "avgData = avgData.drop(columns = 'Stake_ID')\n",
    "avgData = avgData.rename(columns = {'OLS Prediction Ring 5' : 'lai5ring'})\n",
    "\n",
    "## Veg Survey\n",
    "veg_sub = forestInv[['Stake_ID', 'DIST_M', 'DBH_CM', 'DIST_M_SD', 'DBH_CM_SD', 'n', 'basalArea_m2', 'Co', 'Dom', 'Int', 'Sup', 'prop_Coniferous']]\n",
    "peakData = pd.merge(peakData, veg_sub, left_on = 'stakes', right_on = 'Stake_ID')\n",
    "peakData = peakData.drop(columns = 'Stake_ID')\n",
    "peakData = peakData.rename(columns = {'DIST_M' : 'avgDist_m',\n",
    "                                    'DBH_CM' : 'avgDBH_cm',\n",
    "                                    'DIST_M_SD' : 'sdDist_m',\n",
    "                                    'DBH_CM_SD' : 'sdDBH_cm',\n",
    "                                    'n' : 'nTrees',\n",
    "                                    'Co' : 'nCo',\n",
    "                                    'Dom' : 'nDom',\n",
    "                                    'Int' : 'nInt',\n",
    "                                    'Sup' : 'nSup',\n",
    "                                    'prop_Coniferous' : 'pCon'})\n",
    "\n",
    "avgData = pd.merge(avgData, veg_sub, left_on = 'stakes', right_on = 'Stake_ID')\n",
    "avgData = avgData.drop(columns = 'Stake_ID')\n",
    "avgData = avgData.rename(columns = {'DIST_M' : 'avgDist_m',\n",
    "                                    'DBH_CM' : 'avgDBH_cm',\n",
    "                                    'DIST_M_SD' : 'sdDist_m',\n",
    "                                    'DBH_CM_SD' : 'sdDBH_cm',\n",
    "                                    'n' : 'nTrees',\n",
    "                                    'Co' : 'nCo',\n",
    "                                    'Dom' : 'nDom',\n",
    "                                    'Int' : 'nInt',\n",
    "                                    'Sup' : 'nSup',\n",
    "                                    'prop_Coniferous' : 'pCon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melt Data -- need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Melt Data'''\n",
    "melt = pd.read_csv(import_path + 'snowMeltRates.csv')\n",
    "melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Data with other descriptors for analysis\n",
    "meltData = pd.merge(melt[['stakes', 'MaxDate_meas', 'firstSnow_meas', 'lastSnow_meas', 'accumulate_meas', 'melt_meas']],\n",
    "                    peakData,\n",
    "                    on = 'stakes')\n",
    "meltData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 'days' text from columns\n",
    "meltData['melt_meas'] = [int(a[0:-4]) for a in meltData.melt_meas]\n",
    "meltData['accumulate_meas'] = [int(a[0:-4]) for a in meltData.accumulate_meas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Snow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSnow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plot winter snow timeseries\n",
    "fig = plt.figure(constrained_layout = True, \n",
    "    figsize = (9,6))\n",
    "\n",
    "gs = GridSpec(2, 3, figure = fig)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0:2])\n",
    "sns.lineplot(x = allSnow_df.SDOY, y = allSnow_df.depths, hue = allSnow_df.SYear, \n",
    "            palette = [pal[4], pal[2]], ax = ax)\n",
    "ax.set_xlim(80, 250)\n",
    "ax.set_ylim(0, 80)\n",
    "ax.set_xlabel('Date of Snow Year')\n",
    "ax.set_ylabel('Snow Depth, [cm]')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "sns.boxplot(data = allSnow_df, x = 'SYear', y = 'depths',\n",
    "            showbox = False, showcaps = False, \n",
    "            medianprops={\"color\": \"k\", \"linewidth\": 2}, \n",
    "            palette = [pal[4], pal[2]],\n",
    "            ax = ax2, zorder = 1)\n",
    "sns.stripplot(data = allSnow_df, x = 'SYear', y = 'depths', dodge = True, \n",
    "              palette = [pal[4], pal[2]],\n",
    "              ax = ax2, zorder = 0)\n",
    "ax2.set_ylim(0, 80)\n",
    "ax2.set_xlabel('Snow Year')\n",
    "ax2.set_ylabel(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize = (8, 8), \n",
    "                               sharex = True, \n",
    "                               sharey = True, \n",
    "                               layout = 'tight')\n",
    "\n",
    "sns.lineplot(data = allSnow_df[allSnow_df.watershed == 'S2'], x = 'SDOY', y = 'depths', hue = 'SYear',\n",
    "            units = 'stakes',\n",
    "            estimator = None, \n",
    "            palette = [pal[4], pal[2]],\n",
    "            ax = ax1)\n",
    "ax1.set_xlim(80, 250)\n",
    "ax1.set_ylim(0, 80)\n",
    "ax1.set_ylabel('Snow Depth, [cm]')\n",
    "\n",
    "sns.lineplot(data = allSnow_df[allSnow_df.watershed == 'S6'], x = 'SDOY', y = 'depths', hue = 'SYear',\n",
    "            units = 'stakes',\n",
    "            estimator = None, \n",
    "            palette = [pal[4], pal[2]],\n",
    "            ax = ax2)\n",
    "ax2.set_ylabel('Snow Depth, [cm]')\n",
    "ax2.set_xlabel('Date of Snow Year')\n",
    "plt.savefig('./figures/WaterSciCon/snowdepths.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(layout = \"constrained\", figsize = (9, 6))\n",
    "\n",
    "gs = GridSpec(2, 3, figure = fig)\n",
    "ax = fig.add_subplot(gs[0:2, 0:2])\n",
    "sns.boxplot(data = peakData, x = 'zones', y = 'lai5ring',\n",
    "    notch=False, showcaps=False,\n",
    "    flierprops={\"marker\": \"x\"},\n",
    "    boxprops={\"facecolor\": [0.4795847750865052, 0.7984621299500193, 0.7695501730103806, 0.3]},\n",
    "    medianprops={\"color\": \"red\"},\n",
    "    ax = ax)\n",
    "ax.set_xlabel(' ')\n",
    "ax.set_ylabel('5 Ring LAI')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "sns.regplot(data = peakData, x = 'lai5ring', y = 'maxDepth', color = pal[4])\n",
    "results = smf.ols('maxDepth ~ lai5ring', data = peakData).fit()\n",
    "ax2.text(1.7, 75, r'$ p $ = ' + str(round_sig(results.pvalues.lai5ring, 3)))\n",
    "ax2.text(1.7, 72, r'$ R_{adj}^2 $ = ' + str(round_sig(results.rsquared_adj, 3)))\n",
    "\n",
    "ax2.set_xlabel('5 Ring LAI')\n",
    "ax2.set_ylabel('Peak Snow Depth [cm]')\n",
    "ax2.set_xlim(min(peakData.lai5ring), max(peakData.lai5ring))\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 2])\n",
    "sns.regplot(data = meltData, x = 'lai5ring', y = 'accumulate_meas', color = pal[4])\n",
    "results2 = smf.ols('accumulate_meas ~ lai5ring', data = meltData).fit()\n",
    "ax3.text(1.7, 115, r'$ p $ = ' + str(round_sig(results2.pvalues.lai5ring, 3)))\n",
    "ax3.text(1.7, 109, r'$ R_{adj}^2 $ = ' + str(round_sig(results2.rsquared_adj, 3)))\n",
    "\n",
    "ax3.set_xlabel('5 Ring LAI')\n",
    "ax3.set_ylabel('Accumulation Season Length [days]')\n",
    "ax3.set_xlim(min(meltData.lai5ring), max(meltData.lai5ring))\n",
    "\n",
    "#plt.savefig(fig_savepath + 'laicorplot_boxplot.pdf', bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Data\n",
    "bins = list(np.linspace(np.nanmin(peakData.pCon), np.nanmax(peakData.pCon), 4))\n",
    "labels = ['Dense Deciduous', 'Mixed', 'Dense Coniferous']\n",
    "peakData['Bin'] = pd.cut(peakData['pCon'], include_lowest = True, bins = bins, labels = labels)\n",
    "\n",
    "#Plot\n",
    "fig, [ax1, ax2, ax3] = plt.subplots(1, 3, figsize = (9, 3), \n",
    "                                    sharex = True, \n",
    "                                    sharey = True, \n",
    "                                    layout = 'tight')\n",
    "\n",
    "sns.regplot(data = peakData[(peakData.Bin == 'Dense Coniferous') & (peakData.SYear == 2023)], x = 'lai5ring', y = 'maxDepth', \n",
    "            ax = ax1, \n",
    "            color = pal[4], \n",
    "            label = 'High Snow')\n",
    "sns.regplot(data = peakData[(peakData.Bin == 'Dense Coniferous') & (peakData.SYear == 2024)], x = 'lai5ring', y = 'maxDepth', \n",
    "            ax = ax1, \n",
    "            color = pal[2], \n",
    "            label = 'Low Snow')\n",
    "resultsDenseCon_high = smf.ols('maxDepth ~ lai5ring', data = peakData[(peakData.Bin == 'Dense Coniferous') & (peakData.SYear == 2023)]).fit()\n",
    "resultsDenseCon_low = smf.ols('maxDepth ~ lai5ring', data = peakData[(peakData.Bin == 'Dense Coniferous') & (peakData.SYear == 2024)]).fit()\n",
    "ax1.text(0.1, 92, r'$ p $ = ' + str(round_sig(resultsDenseCon_high.pvalues.lai5ring, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(resultsDenseCon_high.rsquared_adj, 2)))\n",
    "ax1.text(0.1, 84, r'$ p $ = ' + str(round_sig(resultsDenseCon_low.pvalues.lai5ring, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(resultsDenseCon_low.rsquared_adj, 2)))\n",
    "ax1.set_title('Coniferous (pCon > 0.66)', fontsize='small', loc='left')\n",
    "ax1.set_ylabel('Peak Snow Depth [cm]')\n",
    "ax1.set_xlabel('LAI')\n",
    "\n",
    "sns.regplot(data = peakData[(peakData.Bin == 'Mixed') & (peakData.SYear == 2023)], x = 'lai5ring', y = 'maxDepth', \n",
    "            ax = ax2, \n",
    "            color = pal[4])\n",
    "sns.regplot(data = peakData[(peakData.Bin == 'Mixed') & (peakData.SYear == 2024)], x = 'lai5ring', y = 'maxDepth', \n",
    "            ax = ax2, \n",
    "            color = pal[2])\n",
    "resultsMixed_high = smf.ols('maxDepth ~ lai5ring', data = peakData[(peakData.Bin == 'Mixed') & (peakData.SYear == 2023)]).fit()\n",
    "resultsMixed_low = smf.ols('maxDepth ~ lai5ring', data = peakData[(peakData.Bin == 'Mixed') & (peakData.SYear == 2024)]).fit()\n",
    "ax2.text(0.1, 92, r'$ p $ = ' + str(round_sig(resultsMixed_high.pvalues.lai5ring, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(resultsMixed_high.rsquared_adj, 2)))\n",
    "ax2.text(0.1, 84, r'$ p $ = ' + str(round_sig(resultsMixed_low.pvalues.lai5ring, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(resultsMixed_low.rsquared_adj, 2)))\n",
    "ax2.set_title('Mixed (0.33 < pCon < 0.66)', fontsize='small', loc='left')\n",
    "ax2.set_ylabel(' ')\n",
    "ax2.set_xlabel('LAI')\n",
    "\n",
    "sns.regplot(data = peakData[(peakData.Bin == 'Dense Deciduous') & (peakData.SYear == 2023)], x = 'lai5ring', y = 'maxDepth', \n",
    "            ax = ax3, \n",
    "            color = pal[4], \n",
    "            label = 'High Snow')\n",
    "sns.regplot(data = peakData[(peakData.Bin == 'Dense Deciduous') & (peakData.SYear == 2024)], x = 'lai5ring', y = 'maxDepth', \n",
    "            ax = ax3, \n",
    "            color = pal[2], \n",
    "            label = 'Low Snow')\n",
    "resultsDenseDec_high = smf.ols('maxDepth ~ lai5ring', data = peakData[(peakData.Bin == 'Dense Deciduous') & (peakData.SYear == 2023)]).fit()\n",
    "resultsDenseDec_low = smf.ols('maxDepth ~ lai5ring', data = peakData[(peakData.Bin == 'Dense Deciduous') & (peakData.SYear == 2024)]).fit()\n",
    "ax3.text(0.1, 92, r'$ p $ = ' + str(round_sig(resultsDenseDec_high.pvalues.lai5ring, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(resultsDenseDec_high.rsquared_adj, 2)))\n",
    "ax3.text(0.1, 84, r'$ p $ = ' + str(round_sig(resultsDenseDec_low.pvalues.lai5ring, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(resultsDenseDec_low.rsquared_adj, 2)))\n",
    "ax3.set_title('Deciduous (pCon < 0.33)', fontsize='small', loc='left')\n",
    "ax3.set_ylabel(' ')\n",
    "ax3.set_xlabel('LAI')\n",
    "\n",
    "ax1.set_xlim(0, max(peakData.lai5ring))\n",
    "ax1.set_ylim(10, 100)\n",
    "ax3.legend(bbox_to_anchor = (1, 1))\n",
    "\n",
    "#plt.savefig(fig_savepath + 'covertype_lairegression.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set PCA Data'''\n",
    "#Switch this depending on average/maximum snow depth considerations\n",
    "pca_data = peakData\n",
    "\n",
    "#remove strings\n",
    "pca_data_forplot = pca_data.drop(columns = ['stakes', 'zones', 'watershed', 'Bin'])\n",
    "\n",
    "#remove depths \n",
    "pca_data = pca_data_forplot.drop(columns = 'maxDepth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Principle Component Analysis to Determine Snow Controls'''\n",
    "#Standardize Data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pca_data)\n",
    "pca_data_scaled = scaler.transform(pca_data)\n",
    "#Convert back to dataframe\n",
    "pca_scaled = pd.DataFrame(data = pca_data_scaled, \n",
    "                            columns = pca_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run PCA\n",
    "pca = PCA(n_components = 10) #adjustable\n",
    "pca.fit_transform(pca_data_scaled)\n",
    "\n",
    "#Extract variance values\n",
    "prop_var = pca.explained_variance_ratio_\n",
    "eigenvalues = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scree Plot\n",
    "PCnumbers = np.arange(pca.n_components_) + 1\n",
    "\n",
    "fig, [ax, ax2] = plt.subplots(1, 2, figsize = (6, 3), \n",
    "                              sharex = True, \n",
    "                              tight_layout = True)\n",
    "\n",
    "ax.plot(PCnumbers, \n",
    "         eigenvalues, \n",
    "         'o-',\n",
    "         color = pal[4])\n",
    "ax.hlines(1, xmax = 0, xmin = 12, colors = 'silver', linestyles = '--', zorder = -1)\n",
    "ax.set_ylabel('Eigenvalue')\n",
    "ax.set_xlabel('Principle Component')\n",
    "\n",
    "ax2.plot(PCnumbers, \n",
    "         np.cumsum(prop_var), \n",
    "         'o-', \n",
    "         color = pal[4])\n",
    "ax2.hlines(0.90, xmax = 0, xmin = 12, colors = 'silver', linestyles = '--', zorder = -1)\n",
    "\n",
    "ax2.set_ylabel('Proportion of Variance')\n",
    "ax2.set_xlabel('Principle Component')\n",
    "\n",
    "ax.set_xlim(0, 11)\n",
    "#plt.savefig(fig_savepath + 'PCAeigenvalues.pdf', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "#Using Kaiser's rule we keep all principle components with eigenvalues above 1 and rerun (here 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerun PCA\n",
    "pca2 = PCA(n_components = 5) #adjustable\n",
    "pc = pca2.fit_transform(pca_data_scaled)\n",
    "\n",
    "pca_snow = pd.DataFrame(data = pc, columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "pca_snow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BiPlot'''\n",
    "def biplot(ax, data, score, coef, ipca1, ipca2, labels=None, leg=None):\n",
    " \n",
    "    xs = score[:, ipca1]\n",
    "    ys = score[:, ipca2]\n",
    "    n = coef.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "\n",
    "    sns.scatterplot(x = jitter(xs * scalex, 0.1), y = jitter(ys * scaley, 0.1), \n",
    "                hue = data.maxDepth, \n",
    "                ax = ax,\n",
    "                palette = 'GnBu',\n",
    "                legend = leg)\n",
    " \n",
    "    for i in range(n):\n",
    "        ax.arrow(0, 0, coef[i, ipca1], \n",
    "                  coef[i, ipca2], color = 'silver',\n",
    "                  alpha = 0.5)\n",
    "        ax.text(coef[i, ipca1]* 1.15, \n",
    "                 coef[i, ipca2] * 1.15, \n",
    "                 labels[i], \n",
    "                 color = 'silver', \n",
    "                 ha = 'center', \n",
    "                 va = 'center')\n",
    " \n",
    "    ax.set_xlabel(\"PC{}\".format(ipca1 + 1))\n",
    "    ax.set_ylabel(\"PC{}\".format(ipca2 + 1)) \n",
    "\n",
    "    ax.set_xlim(-0.7, 0.7)\n",
    "    ax.set_ylim(-0.7, 0.7)\n",
    "\n",
    "    #plt.title('Biplot of PCA')   \n",
    "\n",
    "    #plt.show()\n",
    "\n",
    " \n",
    "fig, axs = plt.subplots(2, 2, figsize = (6.5,5), \n",
    "                        tight_layout = True, \n",
    "                        sharey = True)\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 2):\n",
    "        if((i == 0) & (j == 0)):\n",
    "            biplot(axs[i, j], pca_data_forplot, pc, np.transpose(pca.components_), i+j+1, 0, list(pca_data.columns), True)\n",
    "        else:\n",
    "            biplot(axs[i, j], pca_data_forplot, pc, np.transpose(pca.components_), i+j+1, 0, list(pca_data.columns))\n",
    "\n",
    "axs[0, 0].legend(title = 'Max SD [cm]', bbox_to_anchor = (-0.25, 1))\n",
    "#plt.savefig(fig_savepath + 'PCAplots.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort out PCA coefficients\n",
    "pca_coefs = pd.DataFrame(data = abs(np.transpose(pca2.components_)), columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'], index = pca_data.columns)\n",
    "pca_coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable Importance Plots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n",
    "\n",
    "pca_coefs.plot(kind = 'bar', stacked = True, ax = ax, color = pal)\n",
    "\n",
    "ax.set_xticklabels(['Northing', 'Easting', 'Aspect', 'Slope', 'Snow Year', 'LAI', 'Mean Dist', 'Mean DBH', 'SD Dist', 'SD DBH', '# Trees', 'BA', '# Co', '# Dom', '# Int', '# Sup', '% Conif'])\n",
    "ax.legend(title = 'Component', bbox_to_anchor = (1,1))\n",
    "ax.set_ylabel('Cumulative PCA coefficient')\n",
    "#plt.savefig(fig_savepath + 'PCAcoef.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version weighted by PCA explained percentage\n",
    "pca_coefs_weighted = pca_coefs\n",
    "for i in range(0, 5):\n",
    "    pca_coefs_weighted.iloc[:, i] = pca_coefs_weighted.iloc[:, i]*prop_var[i]\n",
    "\n",
    "pca_coefs_weighted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n",
    "\n",
    "pca_coefs_weighted.plot(kind = 'bar', stacked = True, ax = ax, color = pal)\n",
    "\n",
    "ax.set_xticklabels(['Northing', 'Easting', 'Aspect', 'Slope', 'Snow Year', 'LAI', 'Mean Dist', 'Mean DBH', 'SD Dist', 'SD DBH', '# Trees', 'BA', '# Co', '# Dom', '# Int', '# Sup', '% Conif'])\n",
    "ax.legend(title = 'Component', bbox_to_anchor = (1,1))\n",
    "ax.set_ylabel('Cumulative PCA coefficient, weighted')\n",
    "#plt.savefig(fig_savepath + 'PCAcoef-weighted.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = round(stats.pearsonr(tmp[r], tmp[c])[1], 4)\n",
    "    return pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable intercorrelation\n",
    "pvals = calculate_pvalues(pca_data) \n",
    "sigvals = np.where(pvals < 0.05, pvals, np.nan)\n",
    "\n",
    "sns.heatmap(pca_data.corr(),\n",
    "                xticklabels=pca_data.columns,\n",
    "                yticklabels=pca_data.columns, \n",
    "            cmap = 'vlag', \n",
    "            mask = sigvals, \n",
    "            center = 0, \n",
    "            vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance for Peak Snow Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train_2023, X_test_2023, y_train_2023, y_test_2023 = train_test_split(pca_data[pca_data.SYear == 2023],\n",
    "                                                                        pca_data_forplot[pca_data_forplot.SYear == 2023].maxDepth,\n",
    "                                                                        test_size = 0.5, random_state = 48492)\n",
    "X_train_2024, X_test_2024, y_train_2024, y_test_2024 = train_test_split(pca_data[pca_data.SYear == 2024],\n",
    "                                                                        pca_data_forplot[pca_data_forplot.SYear == 2024].maxDepth,\n",
    "                                                                        test_size = 0.5, random_state = 48492)\n",
    "\n",
    "#Fit random forest regression with 100 trees\n",
    "rf2023 = RandomForestRegressor(n_estimators = 100)\n",
    "rf2023.fit(X_train_2023, y_train_2023)\n",
    "rf2024 = RandomForestRegressor(n_estimators = 100)\n",
    "rf2024.fit(X_train_2024, y_train_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model assessment\n",
    "preds2023 = rf2023.predict(X_test_2023)\n",
    "preds2024 = rf2024.predict(X_test_2024)\n",
    "\n",
    "#Score\n",
    "print('2023 High Snow Model\\n')\n",
    "print(rf2023.score(X_train_2023, y_train_2023))\n",
    "print(rf2023.score(X_test_2023, y_test_2023))\n",
    "\n",
    "print('\\n2024 Low Snow Model\\n')\n",
    "print(rf2024.score(X_train_2024, y_train_2024))\n",
    "print(rf2024.score(X_test_2024, y_test_2024))\n",
    "\n",
    "#Overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort and plot feature importance\n",
    "sorted_idx2023 = rf2023.feature_importances_.argsort()\n",
    "sorted_idx2024 = rf2024.feature_importances_.argsort()\n",
    "\n",
    "fig,[[ax, ax3],[ax2, ax4]] = plt.subplots(2, 2, figsize = (10, 5), \n",
    "                              sharex = True, \n",
    "                              tight_layout = True)\n",
    "\n",
    "#2023\n",
    "#Without permutation\n",
    "ax.barh(pca_data.columns[sorted_idx2023], rf2023.feature_importances_[sorted_idx2023], \n",
    "        color = pal[4])\n",
    "#ax.set_yticklabels(['Aspect', 'Northing', '# Sup', '# Int', 'Mean Dist', '# Trees', 'Easting', 'Slope', 'Mean DBH', '# Dom', 'LAI', 'BA', '% Con', '# Co'])\n",
    "\n",
    "#With permutation\n",
    "perm_importance2023 = permutation_importance(rf2023, X_test_2023, y_test_2023)\n",
    "\n",
    "sorted_idx2023 = perm_importance2023.importances_mean.argsort()\n",
    "ax2.barh(pca_data[pca_data.SYear == 2023].columns[sorted_idx2023], perm_importance2023.importances_mean[sorted_idx2023], \n",
    "         color = pal[4])\n",
    "#ax2.set_yticklabels(['# Dom', '# Sup', '# Int', 'Easting', 'Aspect', 'Mean Dist', 'BA', '# Trees', 'Slope', 'Northing', 'Mean DBH', 'LAI', '% Con', '# Co'])\n",
    "\n",
    "#2024\n",
    "#Without permutation\n",
    "ax3.barh(pca_data.columns[sorted_idx2024], rf2024.feature_importances_[sorted_idx2024], \n",
    "        color = pal[4])\n",
    "#ax.set_yticklabels(['Aspect', 'Northing', '# Sup', '# Int', 'Mean Dist', '# Trees', 'Easting', 'Slope', 'Mean DBH', '# Dom', 'LAI', 'BA', '% Con', '# Co'])\n",
    "\n",
    "#With permutation\n",
    "perm_importance2024 = permutation_importance(rf2024, X_test_2024, y_test_2024)\n",
    "\n",
    "sorted_idx2024 = perm_importance2024.importances_mean.argsort()\n",
    "ax4.barh(pca_data[pca_data.SYear == 2024].columns[sorted_idx2024], perm_importance2024.importances_mean[sorted_idx2024], \n",
    "         color = pal[4])\n",
    "#ax2.set_yticklabels(['# Dom', '# Sup', '# Int', 'Easting', 'Aspect', 'Mean Dist', 'BA', '# Trees', 'Slope', 'Northing', 'Mean DBH', 'LAI', '% Con', '# Co'])\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Random Forest Feature Importance\")\n",
    "ax4.set_xlabel(\"Random Forest Feature Importance\")\n",
    "\n",
    "\n",
    "#plt.savefig(fig_savepath + 'randomForest-importance.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP values\n",
    "explainer2023 = shap.TreeExplainer(rf2023)\n",
    "shap_values2023 = explainer2023.shap_values(X_test_2023)\n",
    "explainer2024 = shap.TreeExplainer(rf2024)\n",
    "shap_values2024 = explainer2024.shap_values(X_test_2024)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "shap.summary_plot(shap_values2023, X_test_2023,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Attribute Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "#ax.set_yticklabels(['# Co', 'Mean DBH', 'pCon', r'/sigma DBH', '# Trees', 'LAI', '# Dom', r'\\sigma Distance', 'Northing', '# Sup', 'Mean Distance', 'Easting', 'Slope', '# Int', 'Aspect', 'Basal Area', 'Year'])\n",
    "ax.set_yticklabels(['Year', 'Basal Area', 'Aspect', '# Int', 'Slope', 'Easting', 'Mean Distance', '# Sup', 'Northing', r'$\\sigma$ Distance', '# Dom', 'LAI', '# Trees', r'$\\sigma$ DBH', 'pCon', 'Mean DBH', '# Co'])\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-highsnow.pdf', bbox_inches = 'tight')\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-highsnow.jpeg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "shap.summary_plot(shap_values2024, X_test_2024,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Attribute Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "#ax.set_yticklabels([r'$\\sigma$ Distance', '# Co', '# Trees', 'pCon', '# Sup', 'Mean DBH', 'Slope', r'$\\sigma$ DBH', 'Mean Distance', 'Aspect', 'Basal Area', 'LAI', '# Dom', 'Easting', 'Northing', '# Int', 'Year'])\n",
    "ax.set_yticklabels(['Year', '# Int', 'Northing', 'Easting', '# Dom', 'LAI', 'Basal Area', 'Aspect', 'Mean Distance', r'$\\sigma$ DBH', 'Slope', 'Mean DBH', '# Sup', 'pCon', '# Trees', '# Co', r'$\\sigma$ Distance' ])\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-lowsnow.pdf', bbox_inches = 'tight')\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-lowsnow.jpeg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance Using Reduced Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train_2023, X_test_2023, y_train_2023, y_test_2023 = train_test_split(pca_data[pca_data.SYear == 2023],\n",
    "                                                                        pca_data_forplot[pca_data_forplot.SYear == 2023].maxDepth,\n",
    "                                                                        test_size = 0.5, random_state = 48492)\n",
    "X_train_2024, X_test_2024, y_train_2024, y_test_2024 = train_test_split(pca_data[pca_data.SYear == 2024],\n",
    "                                                                        pca_data_forplot[pca_data_forplot.SYear == 2024].maxDepth,\n",
    "                                                                        test_size = 0.5, random_state = 48492)\n",
    "\n",
    "#Fit random forest regression with 100 trees\n",
    "rf2023 = RandomForestRegressor(n_estimators = 100)\n",
    "rf2023.fit(X_train_2023, y_train_2023)\n",
    "rf2024 = RandomForestRegressor(n_estimators = 100)\n",
    "rf2024.fit(X_train_2024, y_train_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model assessment\n",
    "preds2023 = rf2023.predict(X_test_2023)\n",
    "preds2024 = rf2024.predict(X_test_2024)\n",
    "\n",
    "#Score\n",
    "print('2023 High Snow Model\\n')\n",
    "print(rf2023.score(X_train_2023, y_train_2023))\n",
    "print(rf2023.score(X_test_2023, y_test_2023))\n",
    "\n",
    "print('\\n2024 Low Snow Model\\n')\n",
    "print(rf2024.score(X_train_2024, y_train_2024))\n",
    "print(rf2024.score(X_test_2024, y_test_2024))\n",
    "\n",
    "#Overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP values\n",
    "explainer2023 = shap.TreeExplainer(rf2023)\n",
    "shap_values2023 = explainer2023.shap_values(X_test_2023)\n",
    "explainer2024 = shap.TreeExplainer(rf2024)\n",
    "shap_values2024 = explainer2024.shap_values(X_test_2024)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "shap.summary_plot(shap_values2023, X_test_2023,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Attribute Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "#ax.set_yticklabels(['# Co', 'Mean DBH', 'pCon', r'/sigma DBH', '# Trees', 'LAI', '# Dom', r'\\sigma Distance', 'Northing', '# Sup', 'Mean Distance', 'Easting', 'Slope', '# Int', 'Aspect', 'Basal Area', 'Year'])\n",
    "ax.set_yticklabels(['Year', 'Basal Area', 'Aspect', '# Int', 'Slope', 'Easting', 'Mean Distance', '# Sup', 'Northing', r'$\\sigma$ Distance', '# Dom', 'LAI', '# Trees', r'$\\sigma$ DBH', 'pCon', 'Mean DBH', '# Co'])\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-highsnow.pdf', bbox_inches = 'tight')\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-highsnow.jpeg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "shap.summary_plot(shap_values2024, X_test_2024,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Attribute Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "#ax.set_yticklabels([r'$\\sigma$ Distance', '# Co', '# Trees', 'pCon', '# Sup', 'Mean DBH', 'Slope', r'$\\sigma$ DBH', 'Mean Distance', 'Aspect', 'Basal Area', 'LAI', '# Dom', 'Easting', 'Northing', '# Int', 'Year'])\n",
    "ax.set_yticklabels(['Year', '# Int', 'Northing', 'Easting', '# Dom', 'LAI', 'Basal Area', 'Aspect', 'Mean Distance', r'$\\sigma$ DBH', 'Slope', 'Mean DBH', '# Sup', 'pCon', '# Trees', '# Co', r'$\\sigma$ Distance' ])\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-lowsnow.pdf', bbox_inches = 'tight')\n",
    "plt.savefig('./figures/WaterSciCon/randomForest-SHAP-lowsnow.jpeg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis by peatland zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_bog = peakData[peakData.zones == 'Bog']\n",
    "\n",
    "#remove strings\n",
    "pca_data_forplot_bog = pca_data_bog.drop(columns = ['stakes', 'zones', 'watershed'])\n",
    "\n",
    "#remove depths \n",
    "pca_data_bog = pca_data_forplot_bog.drop(columns = 'maxDepth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train_bog, X_test_bog, y_train_bog, y_test_bog = train_test_split(pca_data_bog, pca_data_forplot_bog.maxDepth, test_size = 0.5, random_state = 48492)\n",
    "\n",
    "#Fit random forest regression with 100 trees\n",
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(X_train_bog, y_train_bog)\n",
    "\n",
    "#SHAP values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values_bog = explainer.shap_values(X_test_bog)\n",
    "\n",
    "shap.summary_plot(shap_values_bog, X_test_bog,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Snow Depth Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "\n",
    "plt.savefig(fig_savepath + 'randomForest-SHAP-bog.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_upland = peakData[peakData.zones == 'Upland']\n",
    "\n",
    "#remove strings\n",
    "pca_data_forplot_upland = pca_data_upland.drop(columns = ['stakes', 'zones', 'watershed'])\n",
    "\n",
    "#remove depths \n",
    "pca_data_upland = pca_data_forplot_upland.drop(columns = 'maxDepth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train_upland, X_test_upland, y_train_upland, y_test_upland = train_test_split(pca_data_upland, pca_data_forplot_upland.maxDepth, test_size = 0.5, random_state = 48492)\n",
    "\n",
    "#Fit random forest regression with 100 trees\n",
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(X_train_upland, y_train_upland)\n",
    "\n",
    "#SHAP values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values_upland = explainer.shap_values(X_test_upland)\n",
    "\n",
    "shap.summary_plot(shap_values_upland, X_test_upland,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Snow Depth Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "\n",
    "plt.savefig(fig_savepath + 'randomForest-SHAP-upland.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat Analysis for Snow Melt Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_data, meltData.melt_meas, test_size = 0.5, random_state = 48492)\n",
    "\n",
    "#Fit random forest regression with 100 trees\n",
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort and plot feature importance\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "\n",
    "fig, [ax, ax2] = plt.subplots(2, 1, figsize = (6, 5), \n",
    "                              sharex = True, \n",
    "                              tight_layout = True)\n",
    "\n",
    "#Without permutation\n",
    "ax.barh(pca_data.columns[sorted_idx], rf.feature_importances_[sorted_idx], \n",
    "        color = pal[4])\n",
    "#ax.set_yticklabels(['Aspect', 'Northing', '# Sup', '# Int', 'Mean Dist', '# Trees', 'Easting', 'Slope', 'Mean DBH', '# Dom', 'LAI', 'BA', '% Con', '# Co'])\n",
    "\n",
    "#With permutation\n",
    "perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "ax2.barh(pca_data.columns[sorted_idx], perm_importance.importances_mean[sorted_idx], \n",
    "         color = pal[4])\n",
    "#ax2.set_yticklabels(['# Dom', '# Sup', '# Int', 'Easting', 'Aspect', 'Mean Dist', 'BA', '# Trees', 'Slope', 'Northing', 'Mean DBH', 'LAI', '% Con', '# Co'])\n",
    "\n",
    "ax2.set_xlabel(\"Random Forest Feature Importance for Melt Season Length\")\n",
    "\n",
    "plt.savefig(fig_savepath + 'randomForest-importance-melt.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'Melt Rate', \n",
    "                use_log_scale = True, \n",
    "                show = False)\n",
    "\n",
    "plt.savefig(fig_savepath + 'randomForest-SHAP-melt.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canopy Height Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shannon Diversity Index for Canopy Class\n",
    "def calcH(co, dom, int, sup):\n",
    "    tot = np.sum(co + dom + int + sup)\n",
    "\n",
    "    #calc proportions\n",
    "    pCo = co/tot\n",
    "    pDom = dom/tot\n",
    "    pInt = int/tot\n",
    "    pSup = sup/tot\n",
    "\n",
    "    #calc index masking any zeroes\n",
    "    H = np.sum(pCo*np.log(pCo, where = (pCo != 0)) + \n",
    "               pDom*np.log(pDom, where = (pDom != 0)) +\n",
    "               pInt*np.log(pInt, where = (pInt != 0)) + \n",
    "               pSup*np.log(pSup, where = (pSup != 0)))\n",
    "\n",
    "    return -H\n",
    "\n",
    "#Hill-Simpson Diversity Index for Canopy Class\n",
    "def calcS(co, dom, int, sup):\n",
    "    tot = np.sum(co + dom + int + sup)\n",
    "\n",
    "    #calc proportions\n",
    "    pCo = co/tot\n",
    "    pDom = dom/tot\n",
    "    pInt = int/tot\n",
    "    pSup = sup/tot\n",
    "\n",
    "    #calc index masking any zeroes\n",
    "    S = np.sum(pow(pCo, 2) + pow(pDom, 2) + pow(pInt, 2) + pow(pSup, 2))\n",
    "\n",
    "    return 1/S\n",
    "\n",
    "#Proportion of co-dominant trees\n",
    "def calcpCoDom(co, dom, int, sup):\n",
    "    tot = np.sum(co + dom + int + sup)\n",
    "\n",
    "    #calc proportions\n",
    "    pCo = co/tot\n",
    "\n",
    "    return pCo\n",
    "\n",
    "dd = pca_data_forplot\n",
    "dd['pCoDom'] = [calcpCoDom(dd.nCo[i], dd.nDom[i], dd.nInt[i], dd.nSup[i]) for i in dd.index]\n",
    "dd['H'] = [calcH(dd.nCo[i], dd.nDom[i], dd.nInt[i], dd.nSup[i]) for i in dd.index]\n",
    "dd['S'] = [calcS(dd.nCo[i], dd.nDom[i], dd.nInt[i], dd.nSup[i]) for i in dd.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax2, ax3, ax4, ax] = plt.subplots(1, 4, figsize = (12, 3), \n",
    "                              sharey = True, \n",
    "                              tight_layout = True)\n",
    "\n",
    "#Simpson Hill Diversity Index\n",
    "dd['S2'] = [pow(a, 2) for a in dd.S]\n",
    "sns.regplot(x = dd[(dd.SYear == 2023)].S2, y = dd[(dd.SYear == 2023)].maxDepth, order = 2, ax = ax2, color = pal[4])\n",
    "sns.regplot(x = dd[(dd.SYear == 2024)].S2, y = dd[(dd.SYear == 2024)].maxDepth, order = 2, ax = ax2, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results2_high = smf.ols('maxDepth ~ S2', data = dd[(dd.SYear == 2023)]).fit()\n",
    "results2_low = smf.ols('maxDepth ~ S2', data = dd[(dd.SYear == 2024)]).fit()\n",
    "ax2.text(1.2, 92,\n",
    "        r'$ p $ = ' + str(round_sig(results2_high.pvalues.S2, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results2_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax2.text(1.2, 84, \n",
    "        r'$ p $ = ' + str(round_sig(results2_low.pvalues.S2, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results2_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "\n",
    "ax2.set_xlabel('Hill-Simpson Diversity Index')\n",
    "ax2.set_ylabel('Snow Depth [m]')\n",
    "ax2.set_xlim(min(dd.S), max(dd.S))\n",
    "\n",
    "#pCoDom\n",
    "sns.regplot(x = dd[(dd.SYear == 2023)].pCoDom, y = dd[(dd.SYear == 2023)].maxDepth, order = 2, ax = ax3, color = pal[4])\n",
    "sns.regplot(x = dd[(dd.SYear == 2024)].pCoDom, y = dd[(dd.SYear == 2024)].maxDepth, order = 2, ax = ax3, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "dd['pCoDom2'] = [pow(a, 2) for a in dd.pCoDom]\n",
    "results3_high = smf.ols('maxDepth ~ pCoDom2', data = dd[(dd.SYear == 2023)]).fit()\n",
    "results3_low = smf.ols('maxDepth ~ pCoDom2', data = dd[(dd.SYear == 2024)]).fit()\n",
    "ax3.text(0.1, 92, \n",
    "        r'$ p $ = ' + str(round_sig(results3_high.pvalues.pCoDom2, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results3_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax3.text(0.1, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results3_low.pvalues.pCoDom2, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results3_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "\n",
    "ax3.set_xlabel('Proportion of Co-Dominant Trees')\n",
    "ax3.set_ylabel(' ')\n",
    "ax3.set_xlim(min(dd.pCoDom), max(dd.pCoDom))\n",
    "\n",
    "#pCo\n",
    "sns.regplot(x = dd[(dd.SYear == 2023)].pCon, y = dd[(dd.SYear == 2023)].maxDepth, order = 1, ax = ax4, color = pal[4])\n",
    "sns.regplot(x = dd[(dd.SYear == 2024)].pCon, y = dd[(dd.SYear == 2024)].maxDepth, order = 1, ax = ax4, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results4_high = smf.ols('maxDepth ~ pCon', data = dd[(dd.SYear == 2023)]).fit()\n",
    "results4_low = smf.ols('maxDepth ~ pCon', data = dd[(dd.SYear == 2024)]).fit()\n",
    "ax4.text(0.1, 92,\n",
    "        r'$ p $ = ' + str(round_sig(results4_high.pvalues.pCon, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results4_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax4.text(0.1, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results4_low.pvalues.pCon, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results4_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "\n",
    "ax4.set_xlabel('Proportion of Coniferous Trees')\n",
    "ax4.set_ylabel(' ')\n",
    "ax4.set_xlim(min(dd.pCon), max(dd.pCon))\n",
    "\n",
    "#sdDist_cm\n",
    "sns.regplot(x = dd[(dd.SYear == 2023)].sdDist_m, y = dd[(dd.SYear == 2023)].maxDepth, order = 1, ax = ax, color = pal[4])\n",
    "sns.regplot(x = dd[(dd.SYear == 2024)].sdDist_m, y = dd[(dd.SYear == 2024)].maxDepth, order = 1, ax = ax, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results_high = smf.ols('maxDepth ~ sdDist_m', data = dd[(dd.SYear == 2023)]).fit()\n",
    "results_low = smf.ols('maxDepth ~ sdDist_m', data = dd[(dd.SYear == 2024)]).fit()\n",
    "ax.text(1, 92,\n",
    "        r'$ p $ = ' + str(round_sig(results_high.pvalues.sdDist_m, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax.text(1, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results_low.pvalues.sdDist_m, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "\n",
    "ax.set_xlabel('SD of Distance from Plot Center [m]')\n",
    "ax.set_ylabel(' ')\n",
    "ax.set_xlim(min(dd.sdDist_m), max(dd.sdDist_m))\n",
    "ax.set_ylim(10, 100)\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex-SnowDepth.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy Class Diversity Indicator #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCCC(co, coConif, dom, domConif, int, intConif, sup, supConif):\n",
    "    tot = np.sum(co + dom + int + sup)\n",
    "\n",
    "    #calc proportions\n",
    "    pCo = co/tot\n",
    "    pDom = dom/tot\n",
    "    pInt = int/tot\n",
    "    pSup = sup/tot\n",
    "\n",
    "    #calc coniferous proportions\n",
    "    if(co > 0):\n",
    "        pCoConif = coConif/co\n",
    "    else:\n",
    "        pCoConif = 0\n",
    "    if(dom > 0):\n",
    "        pDomConif = domConif/dom\n",
    "    else:\n",
    "        pDomConif = 0\n",
    "    if(int > 0):\n",
    "        pIntConif = intConif/int\n",
    "    else:\n",
    "        pIntConif = 0\n",
    "    if(sup > 0):\n",
    "        pSupConif = supConif/sup\n",
    "    else:\n",
    "        pSupConif = 0\n",
    "\n",
    "    CCC = np.sum(pCo*np.log(pCo, where = (pCo != 0))*pCoConif + \n",
    "               pDom*np.log(pDom, where = (pDom != 0))*pDomConif +\n",
    "               pInt*np.log(pInt, where = (pInt != 0))*pIntConif + \n",
    "               pSup*np.log(pSup, where = (pSup != 0))*pSupConif)\n",
    "\n",
    "\n",
    "    return -CCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved Shannon Diversity Metric -- Includes a proportion of each canopy class that is coniferous \n",
    "\n",
    "#Numerical Representation of Metric\n",
    "Sup = []\n",
    "Int = []\n",
    "Co = []\n",
    "Dom = []\n",
    "SupCon = []\n",
    "IntCon = []\n",
    "CoCon = []\n",
    "DomCon = []\n",
    "H = []\n",
    "S = []\n",
    "CCC = []\n",
    "\n",
    "#Generate fake data for a site\n",
    "nTrees = 10\n",
    "\n",
    "for nSup in np.arange(0, nTrees + 1):\n",
    "    for nInt in np.arange(0, nTrees + 1 - nSup):\n",
    "        for nCo in np.arange(0, nTrees + 1 - nSup - nInt):\n",
    "            nDom = nTrees - nSup - nInt - nCo\n",
    "\n",
    "            for SupConif in np.arange(0, nSup + 1):\n",
    "                for IntConif in np.arange(0, nInt + 1):\n",
    "                    for CoConif in np.arange(0, nCo + 1):\n",
    "                        for DomConif in np.arange(0, nDom + 1):\n",
    "\n",
    "                            #Save data\n",
    "                            Sup.append(nSup)\n",
    "                            Int.append(nInt)\n",
    "                            Co.append(nCo)\n",
    "                            Dom.append(nDom)\n",
    "\n",
    "                            #Save data\n",
    "                            SupCon.append(SupConif)\n",
    "                            IntCon.append(IntConif)\n",
    "                            CoCon.append(CoConif)\n",
    "                            DomCon.append(DomConif)\n",
    "\n",
    "                            #Calculate Shannon Diversity Index\n",
    "                            H.append(calcH(nCo, nDom, nInt, nSup))\n",
    "                            #Calculate Hill Simpson Diversity Index\n",
    "                            S.append(calcS(nCo, nDom, nInt, nSup))\n",
    "                            #Calculate Coniferous Based Canopy Diversity\n",
    "                            CCC.append(calcCCC(nCo, CoConif, nDom, DomConif, nInt, IntConif, nSup, SupConif))                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append into dataframe\n",
    "testData = pd.DataFrame({'Sup' : Sup, \n",
    "                         'Int' : Int, \n",
    "                         'Co' : Co, \n",
    "                         'Dom' : Dom,\n",
    "                         'SupConif' : SupCon, \n",
    "                         'IntConif' : IntCon, \n",
    "                         'CoConif' : CoCon, \n",
    "                         'DomConif' : DomCon, \n",
    "                         'H' : H, \n",
    "                         'S' : S, \n",
    "                         'CCC' : CCC})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(layout=\"constrained\", figsize = (10, 4))\n",
    "\n",
    "gs = GridSpec(1, 3, figure = fig)\n",
    "ax = fig.add_subplot(gs[0, 0:2])\n",
    "sns.scatterplot(data = testData, x = 'Co', y = 'H', ax = ax, label = 'Shannon', zorder = 5, color = pal[2])\n",
    "sns.scatterplot(data = testData, x = 'Co', y = 'S', ax = ax, label = 'Simpson-Hill', color = pal[4])\n",
    "sns.scatterplot(data = testData, x = 'Co', y = 'CCC', ax = ax, label = 'Canopy Class Conif (CCC)', color = 'green')\n",
    "sns.lineplot(data = testData, x = 'Co', y = 'H', ax = ax, label = 'Shannon', zorder = 5, color = pal[2])\n",
    "sns.lineplot(data = testData, x = 'Co', y = 'S', ax = ax, label = 'Simpson-Hill', color = pal[4])\n",
    "sns.lineplot(data = testData, x = 'Co', y = 'CCC', ax = ax, label = 'Canopy Class Conif (CCC)', color = 'green')\n",
    "ax.set_xlabel('Number of Co-dominant trees')\n",
    "ax.set_ylabel('Diversity Index')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "sns.scatterplot(x = jitter(testData['Co'], 0), y = jitter(testData['CoConif'], 0), hue = testData['CCC'], palette = 'Greens', ax = ax2)\n",
    "ax2.set_xlabel('Number of Co-dominant Trees')\n",
    "ax2.set_ylabel('Number of Coniferous Co-dominant Trees')\n",
    "\n",
    "plt.savefig(fig_savepath + 'diversityIndex_option1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete with real data\n",
    "#Data set-up\n",
    "\n",
    "#Import\n",
    "forestDataS2 = pd.read_excel(import_path + 'S2overstory_2023_compiled.xlsx', \n",
    "                             na_values = ['NAN'])\n",
    "forestDataS6 = pd.read_excel(import_path + 'S6overstory_2023_compiled.xlsx', \n",
    "                             na_values = ['NAN'])\n",
    "forestData = pd.concat([forestDataS2, forestDataS6])\n",
    "forestData.head()\n",
    "\n",
    "#Remove Nan rows and strip strings\n",
    "forestData = forestData.dropna(how = 'any')\n",
    "forestData['CC'] = [cl.strip() for cl in forestData.CC]\n",
    "\n",
    "#Label coniferous and deciduous species\n",
    "forestData['Conif'] = np.where(np.isin(forestData.SPECIES,\n",
    "                ['Abies balsamea', 'Picea mariana', 'Larix laricina', 'Picea glauca', 'Pinus resinosa', 'Pinus Strobus']),\n",
    "                'Coniferous', 'Deciduous')\n",
    "\n",
    "#Sort into zones\n",
    "forestData['zones'] = np.where(np.isin(forestData.SITE,\n",
    "                ['S213', 'S221', 'S222', 'S223', 'S233', 'S234', 'S645', 'S634', 'S623', 'S612']), \"Bog\", \n",
    "                np.where(np.isin(forestData.SITE, ['S211', 'S212', 'S224', 'S225', 'S232', 'S235', 'S635', 'S624', 'S613', 'S611', 'S622', 'S633']),\n",
    "                \"Lagg\", \"Upland\"))\n",
    "\n",
    "#Group by site, canopy class, coniferous\n",
    "forestData_group = forestData.groupby(['SITE', 'CC', 'Conif'])['SPECIES'].count().reset_index()\n",
    "\n",
    "#Melt for data to be easier to manage\n",
    "forestData_melt = forestData_group.pivot(index = 'SITE', columns = ['CC', 'Conif'], values = 'SPECIES')\n",
    "forestData_melt = forestData_melt.fillna(0)\n",
    "\n",
    "#Find sums and proportions\n",
    "fs = forestData_melt\n",
    "fs['nCo'] = fs['Co', 'Deciduous'] + fs['Co', 'Coniferous']\n",
    "fs['nDom'] = fs['Dom', 'Deciduous'] + fs['Dom', 'Coniferous']\n",
    "fs['nInt'] = fs['Int', 'Deciduous'] + fs['Int', 'Coniferous']\n",
    "fs['nSup'] = fs['Sup', 'Deciduous'] + fs['Sup', 'Coniferous']\n",
    "\n",
    "#Run diversity index algorithm\n",
    "fs['CCC'] = [calcCCC(fs.nCo[i], fs['Co', 'Coniferous'][i],\n",
    "                    fs.nDom[i], fs['Dom', 'Coniferous'][i],\n",
    "                    fs.nInt[i], fs['Int', 'Coniferous'][i],\n",
    "                    fs.nSup[i], fs['Sup', 'Coniferous'][i]) for i in range(0, len(fs.nCo))]\n",
    "\n",
    "fs = fs.reset_index()\n",
    "\n",
    "fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset and merge with snow data\n",
    "diversityOne = pd.merge(peakSnow[['stakes', 'maxDepth', 'watershed']], fs[['CCC', 'SITE']], left_on = 'stakes', right_on = 'SITE')\n",
    "diversityOne = diversityOne.rename(columns = {diversityOne.columns[3] : 'CCC',\n",
    "                                            diversityOne.columns[4] : 'SITE'})\n",
    "diversityOne.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data and Indicator 1 plot\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "sns.regplot(data = diversityOne[diversityOne.watershed == 'S2'], x = 'CCC', y = 'maxDepth',\n",
    "            ax = ax, color = pal[4], \n",
    "            label = 'S2')\n",
    "sns.regplot(data = diversityOne[diversityOne.watershed == 'S6'], x = 'CCC', y = 'maxDepth',\n",
    "            ax = ax, color = pal[2], \n",
    "            label = 'S6')\n",
    "\n",
    "#Do regression and add to plot\n",
    "resultsS2 = smf.ols('maxDepth ~ CCC', data = diversityOne[diversityOne.watershed == 'S2']).fit()\n",
    "ax.text(0.05, 75, r'$ p_{S2} $ = ' + str(round_sig(resultsS2.pvalues.CCC, 3)))\n",
    "ax.text(0.05, 72, r'$ R_{adj,S2}^2 $ = ' + str(round_sig(resultsS2.rsquared_adj, 3)))\n",
    "\n",
    "resultsS6 = smf.ols('maxDepth ~ CCC', data = diversityOne[diversityOne.watershed == 'S6']).fit()\n",
    "ax.text(0.05, 69, r'$ p_{S6} $ = ' + str(round_sig(resultsS6.pvalues.CCC, 3)))\n",
    "ax.text(0.05, 66, r'$ R_{adj,S6}^2 $ = ' + str(round_sig(resultsS6.rsquared_adj, 3)))\n",
    "\n",
    "ax.set_xlabel('Coniferous Canopy Class Diversity Index')\n",
    "ax.set_ylabel('Snow Depth [cm]')\n",
    "ax.set_xlim(np.nanmin(diversityOne.CCC), np.nanmax(diversityOne.CCC))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex1-SnowDepth.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy Class Diversity Indicator #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCCCa(co, coConif, dom, domConif, int, intConif, sup, supConif):\n",
    "    tot = np.sum(co + dom + int + sup)\n",
    "\n",
    "    #calc proportions\n",
    "    pCo = co/tot\n",
    "    pDom = dom/tot\n",
    "    pInt = int/tot\n",
    "    pSup = sup/tot\n",
    "\n",
    "    #calc coniferout proportions\n",
    "    if(co > 0):\n",
    "        pCoConif = coConif/co\n",
    "    else:\n",
    "        pCoConif = 0\n",
    "    if(dom > 0):\n",
    "        pDomConif = domConif/dom\n",
    "    else:\n",
    "        pDomConif = 0\n",
    "    if(int > 0):\n",
    "        pIntConif = intConif/int\n",
    "    else:\n",
    "        pIntConif = 0\n",
    "    if(sup > 0):\n",
    "        pSupConif = supConif/sup\n",
    "    else:\n",
    "        pSupConif = 0\n",
    "    \n",
    "\n",
    "    CCCa = (pCo*np.log(pCo, where = (pCo != 0))+pCoConif*np.log(pCoConif,\n",
    "                where = (pCoConif != 0))) + pDom*np.log(pDom, where = (pDom != 0))+pDomConif*np.log(pDomConif,\n",
    "                where = (pDomConif != 0)) + pInt*np.log(pInt, where = (pInt != 0))+pIntConif*np.log(pIntConif,\n",
    "                where = (pIntConif != 0)) + pSup*np.log(pSup, where = (pSup != 0))+pSupConif*np.log(pSupConif,\n",
    "                where = (pSupConif != 0))\n",
    "\n",
    "    return -CCCa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved Shannon Diversity Metric -- Includes a proportion of each canopy class that is coniferous \n",
    "\n",
    "#Numerical Representation of Metric\n",
    "Sup = []\n",
    "Int = []\n",
    "Co = []\n",
    "Dom = []\n",
    "SupCon = []\n",
    "IntCon = []\n",
    "CoCon = []\n",
    "DomCon = []\n",
    "H = []\n",
    "S = []\n",
    "CCCa = []\n",
    "\n",
    "#Generate fake data for a site\n",
    "nTrees = 10\n",
    "\n",
    "for nSup in np.arange(0, nTrees + 1):\n",
    "    for nInt in np.arange(0, nTrees + 1 - nSup):\n",
    "        for nCo in np.arange(0, nTrees + 1 - nSup - nInt):\n",
    "            nDom = nTrees - nSup - nInt - nCo\n",
    "\n",
    "            for SupConif in np.arange(0, nSup + 1):\n",
    "                for IntConif in np.arange(0, nInt + 1):\n",
    "                    for CoConif in np.arange(0, nCo + 1):\n",
    "                        for DomConif in np.arange(0, nDom + 1):\n",
    "\n",
    "                            #Save data\n",
    "                            Sup.append(nSup)\n",
    "                            Int.append(nInt)\n",
    "                            Co.append(nCo)\n",
    "                            Dom.append(nDom)\n",
    "\n",
    "                            #Save data\n",
    "                            SupCon.append(SupConif)\n",
    "                            IntCon.append(IntConif)\n",
    "                            CoCon.append(CoConif)\n",
    "                            DomCon.append(DomConif)\n",
    "\n",
    "                            #Calculate Shannon Diversity Index\n",
    "                            H.append(calcH(nCo, nDom, nInt, nSup))\n",
    "                            #Calculate Hill Simpson Diversity Index\n",
    "                            S.append(calcS(nCo, nDom, nInt, nSup))\n",
    "                            #Calculate Coniferous Based Canopy Diversity\n",
    "                            CCCa.append(calcCCCa(nCo, CoConif, nDom, DomConif, nInt, IntConif, nSup, SupConif))                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append into dataframe\n",
    "testData_adapted = pd.DataFrame({'Sup' : Sup, \n",
    "                         'Int' : Int, \n",
    "                         'Co' : Co, \n",
    "                         'Dom' : Dom,\n",
    "                         'SupConif' : SupCon, \n",
    "                         'IntConif' : IntCon, \n",
    "                         'CoConif' : CoCon, \n",
    "                         'DomConif' : DomCon, \n",
    "                         'H' : H, \n",
    "                         'S' : S, \n",
    "                         'CCCa' : CCCa})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(layout=\"constrained\", figsize = (10, 4))\n",
    "\n",
    "gs = GridSpec(1, 3, figure = fig)\n",
    "ax = fig.add_subplot(gs[0, 0:2])\n",
    "sns.scatterplot(data = testData_adapted, x = 'Co', y = 'H', ax = ax, label = 'Shannon', zorder = 5, color = pal[4])\n",
    "sns.scatterplot(data = testData_adapted, x = 'Co', y = 'S', ax = ax, label = 'Simpson-Hill', color = pal[2])\n",
    "sns.scatterplot(data = testData_adapted, x = 'Co', y = 'CCCa', ax = ax, label = 'Canopy Class Conif Adapted (CCCa)', color = 'green')\n",
    "sns.lineplot(data = testData_adapted, x = 'Co', y = 'H', ax = ax, zorder = 5, color = pal[4])\n",
    "sns.lineplot(data = testData_adapted, x = 'Co', y = 'S', ax = ax, color = pal[2])\n",
    "sns.lineplot(data = testData_adapted, x = 'Co', y = 'CCCa', ax = ax, color = 'green')\n",
    "ax.set_xlabel('Number of Co-dominant trees')\n",
    "ax.set_ylabel('Diversity Index')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "sns.scatterplot(x = jitter(testData['Co'], 0), y = jitter(testData_adapted['CoConif'], 0), hue = testData_adapted['CCCa'], palette = 'Greens', ax = ax2)\n",
    "ax2.set_xlabel('Number of Co-dominant Trees')\n",
    "ax2.set_ylabel('Number of Coniferous Co-dominant Trees')\n",
    "\n",
    "plt.savefig(fig_savepath + 'diversityIndex_option2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run diversity index algorithm on real data\n",
    "fs['CCCa'] = [calcCCCa(fs.nCo[i], fs['Co', 'Coniferous'][i],\n",
    "                    fs.nDom[i], fs['Dom', 'Coniferous'][i],\n",
    "                    fs.nInt[i], fs['Int', 'Coniferous'][i],\n",
    "                    fs.nSup[i], fs['Sup', 'Coniferous'][i]) for i in range(0, len(fs.nCo))]\n",
    "\n",
    "fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset and merge with snow data\n",
    "diversityTwo = pd.merge(peakSnow[['stakes', 'maxDepth', 'watershed']], fs[['CCCa', 'SITE']], left_on = 'stakes', right_on = 'SITE')\n",
    "diversityTwo = diversityTwo.rename(columns = {diversityTwo.columns[3] : 'CCCa',\n",
    "                                            diversityTwo.columns[4] : 'SITE'})\n",
    "diversityTwo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data and Indicator 1 plot\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "sns.regplot(data = diversityTwo[diversityTwo.watershed == 'S2'], x = 'CCCa', y = 'maxDepth',\n",
    "            ax = ax, color = pal[4], \n",
    "            label = 'S2')\n",
    "sns.regplot(data = diversityTwo[diversityTwo.watershed == 'S6'], x = 'CCCa', y = 'maxDepth',\n",
    "            ax = ax, color = pal[2], \n",
    "            label = 'S6')\n",
    "\n",
    "#Do regression and add to plot\n",
    "resultsS2 = smf.ols('maxDepth ~ CCCa', data = diversityTwo[diversityTwo.watershed == 'S2']).fit()\n",
    "ax.text(0.05, 75, r'$ p_{S2} $ = ' + str(round_sig(resultsS2.pvalues.CCCa, 3)))\n",
    "ax.text(0.05, 72, r'$ R_{adj,S2}^2 $ = ' + str(round_sig(resultsS2.rsquared_adj, 3)))\n",
    "\n",
    "resultsS6 = smf.ols('maxDepth ~ CCCa', data = diversityTwo[diversityTwo.watershed == 'S6']).fit()\n",
    "ax.text(0.05, 69, r'$ p_{S6} $ = ' + str(round_sig(resultsS6.pvalues.CCCa, 3)))\n",
    "ax.text(0.05, 66, r'$ R_{adj,S6}^2 $ = ' + str(round_sig(resultsS6.rsquared_adj, 3)))\n",
    "\n",
    "ax.set_xlabel('Coniferous Canopy Class Diversity Index')\n",
    "ax.set_ylabel('Snow Depth [cm]')\n",
    "ax.set_xlim(np.nanmin(diversityTwo.CCCa), np.nanmax(diversityTwo.CCCa))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex2-SnowDepth.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Setup but for Uplands'''\n",
    "peakSnow_up = peakSnow[peakSnow.zones == 'Upland']\n",
    "\n",
    "#select upland data\n",
    "forestData_up = forestData[forestData.zones == 'Upland']\n",
    "\n",
    "#Group by site, canopy class, coniferous\n",
    "forestData_up_group = forestData_up.groupby(['SITE', 'CC', 'Conif'])['SPECIES'].count().reset_index()\n",
    "\n",
    "#Melt for data to be easier to manage\n",
    "forestData_up_melt = forestData_up_group.pivot(index = 'SITE', columns = ['CC', 'Conif'], values = 'SPECIES')\n",
    "forestData_up_melt = forestData_up_melt.fillna(0)\n",
    "\n",
    "#Find sums and proportions\n",
    "fs_up = forestData_up_melt\n",
    "fs_up['nCo'] = fs_up['Co', 'Deciduous'] + fs_up['Co', 'Coniferous']\n",
    "fs_up['nDom'] = fs_up['Dom', 'Deciduous'] + fs_up['Dom', 'Coniferous']\n",
    "fs_up['nInt'] = fs_up['Int', 'Deciduous'] + fs_up['Int', 'Coniferous']\n",
    "fs_up['nSup'] = fs_up['Sup', 'Deciduous'] + fs_up['Sup', 'Coniferous']\n",
    "\n",
    "#Run diversity index algorithm\n",
    "fs_up['CCC'] = [calcCCC(fs_up.nCo[i], fs_up['Co', 'Coniferous'][i],\n",
    "                    fs_up.nDom[i], fs_up['Dom', 'Coniferous'][i],\n",
    "                    fs_up.nInt[i], fs_up['Int', 'Coniferous'][i],\n",
    "                    fs_up.nSup[i], fs_up['Sup', 'Coniferous'][i]) for i in range(0, len(fs_up.nCo))]\n",
    "\n",
    "fs_up = fs_up.reset_index()\n",
    "\n",
    "fs_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run diversity index algorithm on real data\n",
    "fs_up['CCCa'] = [calcCCCa(fs_up.nCo[i], fs_up['Co', 'Coniferous'][i],\n",
    "                    fs_up.nDom[i], fs_up['Dom', 'Coniferous'][i],\n",
    "                    fs_up.nInt[i], fs_up['Int', 'Coniferous'][i],\n",
    "                    fs_up.nSup[i], fs_up['Sup', 'Coniferous'][i]) for i in range(0, len(fs_up.nCo))]\n",
    "\n",
    "fs_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset and merge with snow data\n",
    "diversityTwo_up = pd.merge(peakSnow_up[['stakes', 'maxDepth', 'watershed']], fs_up[['CCCa', 'SITE']], left_on = 'stakes', right_on = 'SITE', how = 'inner')\n",
    "diversityTwo_up = diversityTwo_up.rename(columns = {diversityTwo_up.columns[3] : 'CCCa',\n",
    "                                            diversityTwo_up.columns[4] : 'SITE'})\n",
    "diversityTwo_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data and Indicator 1 plot\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "sns.regplot(data = diversityTwo_up[diversityTwo_up.watershed == 'S2'], x = 'CCCa', y = 'maxDepth',\n",
    "            ax = ax, color = pal[4], \n",
    "            label = 'S2')\n",
    "sns.regplot(data = diversityTwo_up[diversityTwo_up.watershed == 'S6'], x = 'CCCa', y = 'maxDepth',\n",
    "            ax = ax, color = pal[2], \n",
    "            label = 'S6')\n",
    "\n",
    "#Do regression and add to plot\n",
    "resultsS2 = smf.ols('maxDepth ~ CCCa', data = diversityTwo_up[diversityTwo_up.watershed == 'S2']).fit()\n",
    "ax.text(0.05, 75, r'$ p_{S2} $ = ' + str(round_sig(resultsS2.pvalues.CCCa, 3)))\n",
    "ax.text(0.05, 72, r'$ R_{adj,S2}^2 $ = ' + str(round_sig(resultsS2.rsquared_adj, 3)))\n",
    "\n",
    "resultsS6 = smf.ols('maxDepth ~ CCCa', data = diversityTwo_up[diversityTwo_up.watershed == 'S6']).fit()\n",
    "ax.text(0.05, 69, r'$ p_{S6} $ = ' + str(round_sig(resultsS6.pvalues.CCCa, 3)))\n",
    "ax.text(0.05, 66, r'$ R_{adj,S6}^2 $ = ' + str(round_sig(resultsS6.rsquared_adj, 3)))\n",
    "\n",
    "ax.set_xlabel('Coniferous Canopy Class Diversity Index')\n",
    "ax.set_ylabel('Snow Depth [cm]')\n",
    "ax.set_xlim(np.nanmin(diversityTwo_up.CCCa), np.nanmax(diversityTwo_up.CCCa))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex2-SnowDepth-uplands.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy Class Indicator #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the proportion of coniferous codominant canopy\n",
    "fs['CCCon'] = fs['Co', 'Coniferous']/fs['nCo']\n",
    "fs['pCo'] = fs['nCo']/(fs['nCo'] + fs['nDom'] + fs['nInt'] + fs['nSup'])\n",
    "fs['CCCon_ntrees'] = fs['Co', 'Coniferous']/(fs['nCo'] + fs['nDom'] + fs['nInt'] + fs['nSup'])\n",
    "fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset and merge with snow data\n",
    "diversityThree = pd.merge(peakData[['stakes', 'maxDepth', 'zones', 'watershed', 'aspect', 'slope', 'lai5ring', 'nTrees', 'pCon', 'SYear']], fs[['CCCon', 'pCo', 'SITE']], left_on = 'stakes', right_on = 'SITE', how = 'inner')\n",
    "diversityThree = diversityThree.rename(columns = {diversityThree.columns[-3] : 'CCCon',\n",
    "                                            diversityThree.columns[-2] : 'pCo', \n",
    "                                            diversityThree.columns[-1] : 'SITE'})\n",
    "diversityThree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate non-dimensional variable\n",
    "diversityThree['PiInd'] = diversityThree.pCon / (diversityThree.lai5ring*diversityThree.pCo*diversityThree.CCCon)\n",
    "diversityThree['logPiInd'] = np.log(diversityThree['PiInd'])\n",
    "\n",
    "#Change infs to nans\n",
    "diversityThree = diversityThree.replace([np.inf, -np.inf], np.nan)\n",
    "diversityThree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data and Indicator 1 plot\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "sns.regplot(data = diversityThree[diversityThree.SYear == 2023], x = 'PiInd', y = 'maxDepth',\n",
    "            logx = True,\n",
    "            ax = ax, color = pal[4])\n",
    "sns.regplot(data = diversityThree[diversityThree.SYear == 2024], x = 'PiInd', y = 'maxDepth',\n",
    "            logx = True,\n",
    "            ax = ax, color = pal[2])\n",
    "\n",
    "#Do regression and add to plot\n",
    "res1 = smf.ols('maxDepth ~ logPiInd', data = diversityThree[(diversityThree.SYear == 2023)]).fit()\n",
    "res2 = smf.ols('maxDepth ~ logPiInd', data = diversityThree[(diversityThree.SYear == 2024)]).fit()\n",
    "ax.text(5, 92,\n",
    "        r'$ p $ = ' + str(round_sig(res1.pvalues.logPiInd, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(res1.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax.text(5, 84, r'$ p $ = ' + str(round_sig(res2.pvalues.logPiInd, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(res2.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "\n",
    "ax.set_xlabel('Coniferous Canopy Class Index')\n",
    "ax.set_ylabel('Snow Depth [cm]')\n",
    "ax.set_xlim(np.nanmin(diversityThree.PiInd), np.nanmax(diversityThree.PiInd))\n",
    "ax.set_ylim(10, 100)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.savefig(fig_savepath + 'DiversityIndex3-SnowDepth.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just Upland Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data and Indicator 1 plot\n",
    "fig, [ax2, ax] = plt.subplots(1, 2, figsize = (7, 3.5), \n",
    "                              layout = 'tight', \n",
    "                              sharex = True, \n",
    "                              sharey = True)\n",
    "\n",
    "sns.regplot(data = diversityThree[diversityThree.zones == 'Upland'], x = 'PiInd', y = 'maxDepth',\n",
    "            logx = True,\n",
    "            ax = ax, color = pal[4])\n",
    "\n",
    "#Do regression and add to plot\n",
    "resultsUp = smf.ols('maxDepth ~ logPiInd', data = diversityThree[diversityThree.zones == 'Upland']).fit()\n",
    "ax.text(18, 38, r'$ p $ = ' + str(round_sig(resultsUp.pvalues.logPiInd, 3)))\n",
    "ax.text(18, 32, r'$ R_{adj}^2 $ = ' + str(round_sig(resultsUp.rsquared_adj, 3)))\n",
    "\n",
    "ax.set_xlabel('Coniferous Canopy Class Index')\n",
    "ax.set_ylabel('Snow Depth [cm]')\n",
    "ax.set_xlim(np.nanmin(diversityThree.PiInd), np.nanmax(diversityThree.PiInd))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.text(0.6, 126, 'Upland Data', color = 'silver')\n",
    "\n",
    "sns.regplot(data = diversityThree, x = 'PiInd', y = 'maxDepth',\n",
    "            logx = True,\n",
    "            ax = ax2, color = pal[4])\n",
    "\n",
    "#Do regression and add to plot\n",
    "results = smf.ols('maxDepth ~ logPiInd', data = diversityThree).fit()\n",
    "ax2.text(18, 38, r'$ p $ = ' + str(round_sig(results.pvalues.logPiInd, 3)))\n",
    "ax2.text(18, 32, r'$ R_{adj}^2 $ = ' + str(round_sig(results.rsquared_adj, 3)))\n",
    "\n",
    "ax2.set_xlabel('Coniferous Canopy Class Index')\n",
    "ax2.set_ylabel('Snow Depth [cm]')\n",
    "ax2.set_xlim(np.nanmin(diversityThree.PiInd), np.nanmax(diversityThree.PiInd))\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.text(0.6, 126, 'All Data', color = 'silver')\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex3-SnowDepth-upland.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsUp.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canopy Class Indicator #3 Melt Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset and merge with snow data\n",
    "diversityThreeMelt = pd.merge(meltData[['stakes', 'melt_meas', 'zones', 'watershed', 'aspect', 'slope', 'lai5ring', 'nTrees', 'pCon']], fs[['CCCon', 'pCo', 'SITE']], left_on = 'stakes', right_on = 'SITE', how = 'inner')\n",
    "diversityThreeMelt = diversityThreeMelt.rename(columns = {diversityThreeMelt.columns[-3] : 'CCCon',\n",
    "                                            diversityThreeMelt.columns[-2] : 'pCo', \n",
    "                                            diversityThreeMelt.columns[-1] : 'SITE'})\n",
    "diversityThreeMelt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate non-dimensional variable\n",
    "diversityThreeMelt['PiInd'] = diversityThreeMelt.pCon / (diversityThreeMelt.lai5ring*diversityThreeMelt.CCCon)\n",
    "diversityThreeMelt['logPiInd'] = np.log(diversityThreeMelt['PiInd'])\n",
    "\n",
    "#Change infs to nans\n",
    "diversityThreeMelt = diversityThreeMelt.replace([np.inf, -np.inf], np.nan)\n",
    "diversityThreeMelt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data and Indicator 1 plot\n",
    "fig, [ax2, ax] = plt.subplots(1, 2, figsize = (7, 3.5), \n",
    "                              layout = 'tight', \n",
    "                              sharex = True, \n",
    "                              sharey = True)\n",
    "\n",
    "sns.regplot(data = diversityThreeMelt[diversityThreeMelt.zones == 'Upland'], x = 'PiInd', y = 'melt_meas',\n",
    "            logx = True,\n",
    "            ax = ax, color = pal[4])\n",
    "\n",
    "#Do regression and add to plot\n",
    "results = smf.ols('melt_meas ~ logPiInd', data = diversityThreeMelt[diversityThreeMelt.zones == 'Upland']).fit()\n",
    "ax.text(15, -15, r'$ p $ = ' + str(round_sig(results.pvalues.logPiInd, 3)))\n",
    "ax.text(15, -22, r'$ R_{adj}^2 $ = ' + str(round_sig(results.rsquared_adj, 3)))\n",
    "\n",
    "ax.set_xlabel('Coniferous Canopy Class Index')\n",
    "ax.set_ylabel('Melt Season Length [days]')\n",
    "ax.set_xlim(np.nanmin(diversityThreeMelt.PiInd), np.nanmax(diversityThreeMelt.PiInd))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.text(0.6, 60, 'Upland Data', color = 'silver')\n",
    "\n",
    "sns.regplot(data = diversityThreeMelt, x = 'PiInd', y = 'melt_meas',\n",
    "            logx = True,\n",
    "            ax = ax2, color = pal[4])\n",
    "\n",
    "#Do regression and add to plot\n",
    "results = smf.ols('melt_meas ~ logPiInd', data = diversityThreeMelt).fit()\n",
    "ax2.text(15, -15, r'$ p $ = ' + str(round_sig(results.pvalues.logPiInd, 3)))\n",
    "ax2.text(15, -22, r'$ R_{adj}^2 $ = ' + str(round_sig(results.rsquared_adj, 3)))\n",
    "\n",
    "ax2.set_xlabel('Coniferous Canopy Class Index')\n",
    "ax2.set_ylabel('Melt Season Length [days]')\n",
    "ax2.set_xlim(np.nanmin(diversityThreeMelt.PiInd), np.nanmax(diversityThreeMelt.PiInd))\n",
    "ax2.set_ylim(-25, 70)\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.text(0.6, 60, 'All Data', color = 'silver')\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex3-Melt-upland.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canopy Class Indicator #3 Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cycle through a select range of trees\n",
    "for ntr in range(1, 11):\n",
    "    #Numerical Representation of Metric\n",
    "    Sup = []\n",
    "    Int = []\n",
    "    Co = []\n",
    "    Dom = []\n",
    "    SupCon = []\n",
    "    IntCon = []\n",
    "    CoCon = []\n",
    "    DomCon = []\n",
    "\n",
    "    #Generate fake data for a site\n",
    "    nTrees = ntr\n",
    "\n",
    "    for nSup in np.arange(0, nTrees + 1):\n",
    "        for nInt in np.arange(0, nTrees + 1 - nSup):\n",
    "            for nCo in np.arange(0, nTrees + 1 - nSup - nInt):\n",
    "                nDom = nTrees - nSup - nInt - nCo\n",
    "\n",
    "                for SupConif in np.arange(0, nSup + 1):\n",
    "                    for IntConif in np.arange(0, nInt + 1):\n",
    "                        for CoConif in np.arange(0, nCo + 1):\n",
    "                            for DomConif in np.arange(0, nDom + 1):\n",
    "\n",
    "                                #Save data\n",
    "                                Sup.append(nSup)\n",
    "                                Int.append(nInt)\n",
    "                                Co.append(nCo)\n",
    "                                Dom.append(nDom)\n",
    "\n",
    "                                #Save data\n",
    "                                SupCon.append(SupConif)\n",
    "                                IntCon.append(IntConif)\n",
    "                                CoCon.append(CoConif)\n",
    "                                DomCon.append(DomConif)\n",
    "\n",
    "    #Append into dataframe\n",
    "    IndicatorData = pd.DataFrame({'Sup' : Sup, \n",
    "                            'Int' : Int, \n",
    "                            'Co' : Co, \n",
    "                            'Dom' : Dom,\n",
    "                            'SupConif' : SupCon, \n",
    "                            'IntConif' : IntCon, \n",
    "                            'CoConif' : CoCon, \n",
    "                            'DomConif' : DomCon})\n",
    "\n",
    "    #Compute Variables\n",
    "    lai_test = np.nanmean(peakData.lai5ring)\n",
    "    IndicatorData['CCCon'] = np.where(np.isnan((IndicatorData['CoConif']/IndicatorData['Co'])), 0, IndicatorData['CoConif']/IndicatorData['Co'])\n",
    "    IndicatorData['pCon'] = (IndicatorData['CoConif'] + IndicatorData['DomConif'] + IndicatorData['IntConif'] + IndicatorData['SupConif'])/(IndicatorData['Co'] + IndicatorData['Dom'] + IndicatorData['Int'] + IndicatorData['Sup'])\n",
    "    IndicatorData['pCo'] = IndicatorData['Co']/(IndicatorData['Co'] + IndicatorData['Dom'] + IndicatorData['Int'] + IndicatorData['Sup'])\n",
    "    IndicatorData['PiInd'] = IndicatorData.pCon / (lai_test*IndicatorData.CCCon)\n",
    "\n",
    "    #Change infs to nans\n",
    "    IndicatorData = IndicatorData.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    #Plot\n",
    "    temp = IndicatorData[(IndicatorData.pCo > 0)]\n",
    "    sns.relplot(data = temp, x = jitter(temp['pCon'], 0.1), y = jitter(temp['CCCon'], 0.1),\n",
    "                hue = 'PiInd', palette = 'Greens', \n",
    "                col = 'pCo', col_wrap = 5, \n",
    "                height = 3, aspect = 1)\n",
    "\n",
    "    plt.savefig(fig_savepath + 'simulationfigs/IndicatorModel_nTrees_' + str(ntr) + '.pdf', \n",
    "                bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulations with uniform tree numbers (n = 10) and lai set to average of MEF sites\n",
    "fig, axs = plt.subplots(2, 5, figsize = (11, 5), \n",
    "                        sharex = True, \n",
    "                        sharey = True, \n",
    "                        layout = 'tight')\n",
    "\n",
    "co = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "for i in range(0, 10):\n",
    "    temp2 = temp[temp.pCo == co[i]]\n",
    "\n",
    "    #Index Values\n",
    "    yVal = temp2['pCon']/(temp2['CCCon'])\n",
    "    \n",
    "    #Model Snow Depth values\n",
    "    temp2['SD'] = resultsUp.params.Intercept + resultsUp.params.logPiInd*np.log(temp2.PiInd)\n",
    "    c = sns.scatterplot(data = temp2, x = jitter(yVal, 0.1), y = jitter(temp2['SD'], 0.1),\n",
    "                color = pal[4], ax = axs.ravel()[i],\n",
    "                linewidth = 0, alpha = 0.3, label = 'Snow Depth')\n",
    "    d = sns.regplot(data = temp2, x = jitter(yVal, 0.1), y = jitter(temp2['SD'], 0.1),\n",
    "                color = pal[4], ax = axs.ravel()[i],\n",
    "                lowess = True, scatter = False)\n",
    "    axs.ravel()[i].set_ylim(30, 70)\n",
    "\n",
    "    #remove duplicate secondary y axes\n",
    "    axs.ravel()[i].set_ylabel('Modelled Snow Depth [cm]')\n",
    "        \n",
    "    axs.ravel()[i].set_title('pCo = ' + str(co[i]), fontsize='small', loc='left')\n",
    "\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex3-Simulations.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulations with uniform tree numbers (n = 10), pCo at 0.5, and a range of LAI\n",
    "fig, axs = plt.subplots(1, 5, figsize = (11, 3), \n",
    "                        sharex = True, \n",
    "                        sharey = True, \n",
    "                        layout = 'tight')\n",
    "\n",
    "co = 0.5\n",
    "temp2 = temp[temp.pCo == co]\n",
    "lai = np.linspace(start = np.nanmin(peakData.lai5ring), stop = np.nanmax(peakData.lai5ring), num = 5)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    temp2.PiInd = temp2.pCon / (lai[i]*temp2.CCCon)\n",
    "    yVal = temp2.pCon / (temp2.CCCon)\n",
    "\n",
    "    #Index Values\n",
    "    a = sns.scatterplot(data = temp2, x = jitter(yVal, 0.1), y = jitter(temp2['PiInd'], 0.1),\n",
    "                color = pal[2], ax = axs.ravel()[i], \n",
    "                linewidth = 0, alpha = 0.3, label = 'CCC Index')\n",
    "    b = sns.regplot(data = temp2, x = jitter(yVal, 0.1), y = jitter(temp2['PiInd'], 0.1),\n",
    "                color = pal[2], ax = axs.ravel()[i], \n",
    "                lowess = True, scatter = False)\n",
    "    \n",
    "    #Model Snow Depth values\n",
    "    temp2['SD'] = resultsUp.params.Intercept + resultsUp.params.logPiInd*np.log(temp2.PiInd)\n",
    "    ax2 = axs.ravel()[i].twinx()\n",
    "    c = sns.scatterplot(data = temp2, x = jitter(yVal, 0.1), y = jitter(temp2['SD'], 0.1),\n",
    "                color = pal[4], ax = ax2,\n",
    "                linewidth = 0, alpha = 0.3, label = 'Snow Depth')\n",
    "    d = sns.regplot(data = temp2, x = jitter(yVal, 0.1), y = jitter(temp2['SD'], 0.1),\n",
    "                color = pal[4], ax = ax2,\n",
    "                lowess = True, scatter = False)\n",
    "    ax2.set_ylim(30, 100)\n",
    "\n",
    "    #remove duplicate secondary y axes\n",
    "    if(i%5 != 4):\n",
    "        ax2.set_axis_off()\n",
    "    else:\n",
    "        ax2.set_ylabel('Modelled Snow Depth [cm]')\n",
    "        \n",
    "    #add labels \n",
    "    if(i != 0):\n",
    "        axs.ravel()[i].get_legend().remove()\n",
    "        ax2.get_legend().remove()\n",
    "    \n",
    "    axs.ravel()[i].set_yscale('log')\n",
    "    axs.ravel()[i].set_xlabel(r'$pCCC$', fontsize = 'large')\n",
    "    axs.ravel()[i].set_ylabel('CCC Index')\n",
    "    axs.ravel()[i].set_title('LAI = ' + str(round(lai[i], 3)), fontsize='small', loc='left')\n",
    "\n",
    "\n",
    "plt.savefig(fig_savepath + 'DiversityIndex3-Simulations-varyLAI.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndicatorData['pCCC'] = IndicatorData['pCon']/(IndicatorData['CCCon'])\n",
    "IndicatorData = IndicatorData.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "sns.scatterplot(x = jitter(IndicatorData['Co'], 0.1), y = jitter(IndicatorData['CoConif'], 0.1),\n",
    "                 hue = IndicatorData['pCCC'], palette = 'Greens', \n",
    "                 ax = ax)\n",
    "\n",
    "ax.set_xlabel('Number of Co-dominant Trees')\n",
    "ax.set_ylabel('Number of Coniferous Co-dominant Trees')\n",
    "plt.legend(title = r'$pCCC}$')\n",
    "\n",
    "plt.savefig(fig_savepath + \"DiversityIndex3-Simulations-CCCfrac.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy Class Inidicator #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize = (6, 3), \n",
    "                               layout = 'tight', \n",
    "                               sharey = True)\n",
    "\n",
    "#Proportion of co-dominant trees\n",
    "sns.regplot(x = dd[(dd.SYear == 2023)].pCoDom, y = dd[(dd.SYear == 2023)].maxDepth, order = 2, ax = ax1, color = pal[4])\n",
    "sns.regplot(x = dd[(dd.SYear == 2024)].pCoDom, y = dd[(dd.SYear == 2024)].maxDepth, order = 2, ax = ax1, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results3_high = smf.ols('maxDepth ~ pCoDom2', data = dd[(dd.SYear == 2023)]).fit()\n",
    "results3_low = smf.ols('maxDepth ~ pCoDom2', data = dd[(dd.SYear == 2024)]).fit()\n",
    "ax1.text(0.1, 92, \n",
    "        r'$ p $ = ' + str(round_sig(results3_high.pvalues.pCoDom2, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results3_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax1.text(0.1, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results3_low.pvalues.pCoDom2, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results3_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "\n",
    "ax1.set_xlabel('Proportion of Co-Dominant Trees')\n",
    "ax1.set_ylabel(' ')\n",
    "ax1.set_xlim(min(dd.pCoDom), max(dd.pCoDom))\n",
    "ax1.set_ylim(10, 100)\n",
    "\n",
    "#Diversity Index4\n",
    "dd['CoH_Index'] = (np.log(dd.pCon + 1))*(dd.pCoDom2)\n",
    "dd = dd.replace([np.inf, -np.inf], np.nan)\n",
    "sns.regplot(x = dd[(dd.SYear == 2023)].CoH_Index, y = dd[(dd.SYear == 2023)].maxDepth, order = 2, ax = ax2, color = pal[4])\n",
    "sns.regplot(x = dd[(dd.SYear == 2024)].CoH_Index, y = dd[(dd.SYear == 2024)].maxDepth, order = 2, ax = ax2, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results2_high = smf.ols('maxDepth ~ CoH_Index', data = dd[(dd.SYear == 2023)]).fit()\n",
    "results2_low = smf.ols('maxDepth ~ CoH_Index', data = dd[(dd.SYear == 2024)]).fit()\n",
    "ax2.text(0.1, 92, \n",
    "        r'$ p $ = ' + str(round_sig(results2_high.pvalues.CoH_Index, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results2_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax2.text(0.1, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results2_low.pvalues.CoH_Index, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results2_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "#ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Diversity Index 4')\n",
    "ax2.set_ylabel(' ')\n",
    "ax2.set_xlim(np.nanmin(dd.CoH_Index), np.nanmax(dd.CoH_Index))\n",
    "ax2.set_ylim(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversityFour = pd.merge(peakData[['stakes', 'maxDepth', 'zones', 'watershed', 'aspect', 'slope', 'lai5ring', 'nTrees', 'pCon', 'SYear']], fs[['CCCon', 'CCCon_ntrees', 'pCo', 'SITE']], left_on = 'stakes', right_on = 'SITE', how = 'inner')\n",
    "diversityFour = diversityFour.rename(columns = {diversityFour.columns[-4] : 'CCCon',\n",
    "                                            diversityFour.columns[-3] : 'CCCon_tree',\n",
    "                                            diversityFour.columns[-2] : 'pCo', \n",
    "                                            diversityFour.columns[-1] : 'SITE'})\n",
    "diversityFour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate non-dimensional variable\n",
    "diversityFour['Ind'] = (diversityFour.lai5ring*(np.sqrt(diversityFour.pCon))*(diversityFour.pCo*diversityFour.pCo))\n",
    "diversityFour['IndLog'] = (diversityFour.lai5ring*(np.log(diversityFour.pCon + 1))*(diversityFour.pCo*diversityFour.pCo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversityFour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Representation of Metric\n",
    "Sup = []\n",
    "Int = []\n",
    "Co = []\n",
    "Dom = []\n",
    "SupCon = []\n",
    "IntCon = []\n",
    "CoCon = []\n",
    "DomCon = []\n",
    "H = []\n",
    "S = []\n",
    "divFour = []\n",
    "\n",
    "#Generate fake data for a site\n",
    "nTrees = 10\n",
    "\n",
    "for nSup in np.arange(0, nTrees + 1):\n",
    "    for nInt in np.arange(0, nTrees + 1 - nSup):\n",
    "        for nCo in np.arange(0, nTrees + 1 - nSup - nInt):\n",
    "            nDom = nTrees - nSup - nInt - nCo\n",
    "\n",
    "            for SupConif in np.arange(0, nSup + 1):\n",
    "                for IntConif in np.arange(0, nInt + 1):\n",
    "                    for CoConif in np.arange(0, nCo + 1):\n",
    "                        for DomConif in np.arange(0, nDom + 1):\n",
    "\n",
    "                            totConif = SupConif + IntConif + CoConif + DomConif\n",
    "\n",
    "                            #Save data\n",
    "                            Sup.append(nSup)\n",
    "                            Int.append(nInt)\n",
    "                            Co.append(nCo)\n",
    "                            Dom.append(nDom)\n",
    "\n",
    "                            #Save data\n",
    "                            SupCon.append(SupConif)\n",
    "                            IntCon.append(IntConif)\n",
    "                            CoCon.append(CoConif)\n",
    "                            DomCon.append(DomConif)\n",
    "\n",
    "                            #Calculate Fourth Diversity Index (Square Root)\n",
    "                            divFour.append((np.nanmean(diversityFour.lai5ring)*(np.sqrt(totConif/nTrees))*((nCo/nTrees)*(nCo/nTrees))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append into dataframe\n",
    "divFour_modelled = pd.DataFrame({'Sup' : Sup, \n",
    "                         'Int' : Int, \n",
    "                         'Co' : Co, \n",
    "                         'Dom' : Dom,\n",
    "                         'SupConif' : SupCon, \n",
    "                         'IntConif' : IntCon, \n",
    "                         'CoConif' : CoCon, \n",
    "                         'DomConif' : DomCon, \n",
    "                         'divFour' : divFour})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "\n",
    "sns.scatterplot(x = jitter(divFour_modelled['Co'], 0.1), y = jitter(divFour_modelled['CoConif'], 0.1),\n",
    "                 hue = divFour_modelled['divFour'], palette = 'Greens', \n",
    "                 ax = ax)\n",
    "\n",
    "ax.set_xlabel('Number of Co-dominant Trees')\n",
    "ax.set_ylabel('Number of Coniferous Co-dominant Trees')\n",
    "plt.legend(title = r'$\\Pi_2}$')\n",
    "\n",
    "plt.savefig(fig_savepath + \"DiversityIndex4-Simulations-SqrtPI.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy Class Index #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax, ax2, ax3] = plt.subplots(1, 3, figsize = (9, 3), \n",
    "                                             sharey = True, \n",
    "                                             layout = 'tight')\n",
    "\n",
    "#Diversity Three (Pi Theorem Index)\n",
    "sns.regplot(data = diversityThree[diversityThree.SYear == 2023], x = 'PiInd', y = 'maxDepth',\n",
    "            logx = True,\n",
    "            ax = ax, color = pal[4])\n",
    "sns.regplot(data = diversityThree[diversityThree.SYear == 2024], x = 'PiInd', y = 'maxDepth',\n",
    "            logx = True,\n",
    "            ax = ax, color = pal[2])\n",
    "res1 = smf.ols('maxDepth ~ logPiInd', data = diversityThree[(diversityThree.SYear == 2023)]).fit()\n",
    "res2 = smf.ols('maxDepth ~ logPiInd', data = diversityThree[(diversityThree.SYear == 2024)]).fit()\n",
    "ax.text(-0.5, 92,\n",
    "        r'$ p $ = ' + str(round_sig(res1.pvalues.logPiInd, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(res1.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax.text(-0.5, 84, r'$ p $ = ' + str(round_sig(res2.pvalues.logPiInd, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(res2.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "ax.set_xlim(np.nanmin(diversityThree.logPiInd), np.nanmax(diversityThree.logPiInd))\n",
    "ax.text(-0.5, 76, r'$ n $ = ' + str(len(res1.fittedvalues) + len(res2.fittedvalues)))\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Snow Depth [cm]')\n",
    "ax.set_title('Pi Theorem Index (log)', fontsize='small', loc='left')\n",
    "\n",
    "#Diversity Four Square Root\n",
    "sns.regplot(x = diversityFour[(diversityFour.SYear == 2023)].Ind, y = diversityFour[(diversityFour.SYear == 2023)].maxDepth, ax = ax2, color = pal[4])\n",
    "sns.regplot(x = diversityFour[(diversityFour.SYear == 2024)].Ind, y = diversityFour[(diversityFour.SYear == 2024)].maxDepth, ax = ax2, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results_high = smf.ols('maxDepth ~ Ind', data = diversityFour[(diversityFour.SYear == 2023)]).fit()\n",
    "results_low = smf.ols('maxDepth ~ Ind', data = diversityFour[(diversityFour.SYear == 2024)]).fit()\n",
    "ax2.text(0.05, 92, \n",
    "        r'$ p $ = ' + str(round_sig(results_high.pvalues.Ind, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax2.text(0.05, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results_low.pvalues.Ind, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "ax2.text(0.05, 76, r'$ n $ = ' + str(len(results_high.fittedvalues) + len(results_low.fittedvalues)))\n",
    "ax2.set_xlim(np.nanmin(diversityFour.Ind), np.nanmax(diversityFour.Ind))\n",
    "ax2.set_ylabel(' ')\n",
    "ax2.set_xlabel('Index')\n",
    "ax2.set_title('Regression Index - Square root', fontsize='small', loc='left')\n",
    "\n",
    "#Diversity Four Logarithm\n",
    "sns.regplot(x = diversityFour[(diversityFour.SYear == 2023)].IndLog, y = diversityFour[(diversityFour.SYear == 2023)].maxDepth, ax = ax3, color = pal[4])\n",
    "sns.regplot(x = diversityFour[(diversityFour.SYear == 2024)].IndLog, y = diversityFour[(diversityFour.SYear == 2024)].maxDepth, ax = ax3, color = pal[2])\n",
    "#Do regression and add to plot\n",
    "results2_high = smf.ols('maxDepth ~ IndLog', data = diversityFour[(diversityFour.SYear == 2023)]).fit()\n",
    "results2_low = smf.ols('maxDepth ~ IndLog', data = diversityFour[(diversityFour.SYear == 2024)]).fit()\n",
    "ax3.text(0.05, 92, \n",
    "        r'$ p $ = ' + str(round_sig(results2_high.pvalues.IndLog, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results2_high.rsquared_adj, 2)), \n",
    "        color = pal[4])\n",
    "ax3.text(0.05, 84,\n",
    "        r'$ p $ = ' + str(round_sig(results2_low.pvalues.IndLog, 2)) + r', $ R_{adj}^2 $ = ' + str(round_sig(results2_low.rsquared_adj, 2)), \n",
    "        color = pal[2])\n",
    "ax3.text(0.05, 76, r'$ n $ = ' + str(len(results2_high.fittedvalues) + len(results2_low.fittedvalues)))\n",
    "ax3.set_xlim(np.nanmin(diversityFour.IndLog), np.nanmax(diversityFour.IndLog))\n",
    "ax3.set_ylabel(' ')\n",
    "ax3.set_xlabel('Index')\n",
    "ax3.set_title('Regression Index - Log', fontsize='small', loc='left')\n",
    "\n",
    "ax.set_ylim(10, 100)\n",
    "\n",
    "plt.savefig(fig_savepath + 'index_summary.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this says to me is that snow depth is more dependent on the interaction of LAI and tree structure in dense coniferous canopies -- whereas LAI is a perfectly fine predictor in deciduous canopies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it all with peak SWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this, how do we know what we are seeing isn't just the effects of shading? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = pd.read_csv(import_path + '2023SWE_data.csv')\n",
    "swe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakDataSWE = swe.groupby(['SITE'])['SWE_cm'].max().reset_index()\n",
    "peakDataSWE = peakDataSWE.rename(columns = {'depths' : 'maxDepth'})\n",
    "\n",
    "lai_sub = lai_df[['Stake_ID', 'OLS Prediction Ring 5']]\n",
    "peakDataSWE = pd.merge(peakDataSWE, lai_sub, left_on = 'SITE', right_on = 'Stake_ID')\n",
    "peakDataSWE = peakDataSWE.drop(columns = 'Stake_ID')\n",
    "peakDataSWE = peakDataSWE.rename(columns = {'OLS Prediction Ring 5' : 'lai5ring'})\n",
    "\n",
    "veg_sub = forestInv[['Stake_ID', 'DIST_M', 'DBH_CM', 'DIST_M_SD', 'DBH_CM_SD', 'n', 'basalArea_m2', 'Co', 'Dom', 'Int', 'Sup', 'prop_Coniferous']]\n",
    "peakDataSWE = pd.merge(peakDataSWE, veg_sub, left_on = 'SITE', right_on = 'Stake_ID')\n",
    "peakDataSWE = peakDataSWE.drop(columns = 'Stake_ID')\n",
    "peakDataSWE = peakDataSWE.rename(columns = {'DIST_M' : 'avgDist_m',\n",
    "                                    'DBH_CM' : 'avgDBH_cm',\n",
    "                                    'DIST_M_SD' : 'sdDist_m',\n",
    "                                    'DBH_CM_SD' : 'sdDBH_cm',\n",
    "                                    'n' : 'nTrees',\n",
    "                                    'Co' : 'nCo',\n",
    "                                    'Dom' : 'nDom',\n",
    "                                    'Int' : 'nInt',\n",
    "                                    'Sup' : 'nSup',\n",
    "                                    'prop_Coniferous' : 'pCon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakDataSWE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set PCA Data'''\n",
    "#Switch this depending on average/maximum snow depth considerations\n",
    "pca_data = peakDataSWE\n",
    "\n",
    "#remove strings\n",
    "pca_data_forplot = pca_data.drop(columns = ['SITE'])\n",
    "\n",
    "#remove depths \n",
    "pca_data = pca_data_forplot.drop(columns = 'SWE_cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train_SWE, X_test_SWE, y_train_SWE, y_test_SWE = train_test_split(pca_data, pca_data_forplot.SWE_cm,\n",
    "                                                                        test_size = 0.5, random_state = 48492)\n",
    "\n",
    "#Fit random forest regression with 100 trees\n",
    "rfSWE = RandomForestRegressor(n_estimators = 100)\n",
    "rfSWE.fit(X_train_SWE, y_train_SWE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idxSWE = rfSWE.feature_importances_.argsort()\n",
    "\n",
    "fig,[ax, ax2] = plt.subplots(2, 1, figsize = (6, 5), \n",
    "                              sharex = True, \n",
    "                              tight_layout = True)\n",
    "\n",
    "#2023\n",
    "#Without permutation\n",
    "ax.barh(pca_data.columns[sorted_idxSWE], rfSWE.feature_importances_[sorted_idxSWE], \n",
    "        color = pal[4])\n",
    "#ax.set_yticklabels(['Aspect', 'Northing', '# Sup', '# Int', 'Mean Dist', '# Trees', 'Easting', 'Slope', 'Mean DBH', '# Dom', 'LAI', 'BA', '% Con', '# Co'])\n",
    "\n",
    "#With permutation\n",
    "perm_importanceSWE = permutation_importance(rfSWE, X_test_SWE, y_test_SWE)\n",
    "\n",
    "sorted_idxSWE = perm_importanceSWE.importances_mean.argsort()\n",
    "ax2.barh(pca_data.columns[sorted_idxSWE], perm_importanceSWE.importances_mean[sorted_idxSWE], \n",
    "         color = pal[4])\n",
    "#ax2.set_yticklabels(['# Dom', '# Sup', '# Int', 'Easting', 'Aspect', 'Mean Dist', 'BA', '# Trees', 'Slope', 'Northing', 'Mean DBH', 'LAI', '% Con', '# Co'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainerSWE = shap.TreeExplainer(rfSWE)\n",
    "shap_valuesSWE = explainerSWE.shap_values(X_test_SWE)\n",
    "\n",
    "\n",
    "shap.summary_plot(shap_valuesSWE, X_test_SWE,\n",
    "                cmap = 'GnBu', \n",
    "                plot_size = (5, 5), \n",
    "                color_bar_label = 'SWE Value', \n",
    "                use_log_scale = True, \n",
    "                show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture and Drydown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Drydown data\n",
    "drydown = pd.read_csv(import_path + 'soilMoisture_Drawdown.csv')\n",
    "drydown.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with veg data and the regression index data\n",
    "drydown_veg = pd.merge(drydown, diversityFour, right_on = 'SITE', left_on = 'Site', how = 'inner')\n",
    "drydown_veg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax, ax2, ax3] = plt.subplots(1, 3, figsize = (11, 4), \n",
    "                            sharex = True, \n",
    "                            layout = 'tight')\n",
    "\n",
    "sns.violinplot(data = drydown_veg, x = 'Bin', y = 'BeginDOY',\n",
    "            color = 'white', ax = ax)\n",
    "sns.stripplot(data = drydown_veg, x = 'Bin', y = 'BeginDOY',\n",
    "            color = pal[3], ax = ax,\n",
    "            zorder = 4)\n",
    "ax.set_xlabel('Canopy Classification')\n",
    "ax.set_ylabel('Beginning of Summer Drawdown, Day of Year')\n",
    "\n",
    "sns.violinplot(data = drydown_veg, x = 'Bin', y = 'Drawdown',\n",
    "            color = 'white', ax = ax2)\n",
    "sns.stripplot(data = drydown_veg, x = 'Bin', y = 'Drawdown',\n",
    "            color = pal[3], ax = ax2,\n",
    "            zorder = 4)\n",
    "ax2.set_xlabel('Canopy Classification')\n",
    "ax2.set_ylabel('Soil Moisture Drawdown [cm3/cm3]')\n",
    "\n",
    "sns.violinplot(data = drydown_veg, x = 'Bin', y = 'EndAmt',\n",
    "            color = 'white', ax = ax3)\n",
    "sns.stripplot(data = drydown_veg, x = 'Bin', y = 'EndAmt',\n",
    "            color = pal[3], ax = ax3,\n",
    "            zorder = 4)\n",
    "ax3.set_xlabel('Canopy Classification')\n",
    "ax3.set_ylabel('End of Summer Soil Moisture [cm3/cm3]')\n",
    "\n",
    "plt.savefig(fig_savepath + 'drawdown_violinplots.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
